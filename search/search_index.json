{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation for the Rokita Lab and BTI Bioinformatics Core Welcome to ReadtheDocs for Children's National Hospital Rokita Lab and Brain Tumor Institute (BTI) Bioinformatics team members. This website is a centralized location that contains information about tool accessibility as well as general guidance for utilizing compute infrastructure and using reproducible practices for analytical projects. To file an issue, please see the docs repo here . Responsible and Ethical Use of Bioinformatics Resources Ethical Considerations Please be sure to work with your PI and/or Dr. Rokita to understand which metadata, clinical data, and genomic data types are allowed to be stored in public and private spaces. Please do not share any passwords, tokens, account numbers (e.g. AWS), or IP addresses externally or within GitHub comments, issues, or commits. If you have questions, please ask us! Submitting Jobs or Tasks HPC: When utilizing the HPC, please be mindful when submitting large or many jobs, as this is a community resource. AWS: We have created an EC2 service catalog product, \"BTI Research\" for use by researchers in the BTI. Each person can utlize their own instance without affecting others' compute. CAVATICA : We primarily utilize CAVATICA for established upstream workflows. Jobs will be automatically scheduled and queued based on your user capacity. Fiscal Responsibility AWS: Please only launch an EC2 instance type and volume for the work you will need to do to minimize costs (We are charged for instances per hour and by storage). Below are some ways you can minimize costs. Always STOP your instance once you are finished, or if are not actively working on the instance (e.g. you have 3 meetings and will not be working on it - you can stop it and come back later). TERMINATE your instance once you no longer need it, or if you will not be using it for an extended period of time (weeks - months). CAVATICA: Use of CAVATICA requires set up of a billing group and funds. Please work with your PI and/or Dr. Rokita if you would like to run tasks in CAVATICA. The NIH has Cloud Credits available for working with specific datasets through application (e.g. CFDE and/or Kids First ). Cancer Genomics Cloud (CGC) : New users of the CGC get $300 in cloud credits when they create a new account, and this can be used to run and/or test new workflows as desired. CGC has cloud credits available through application as well. Responsible Use of Artificial Intelligence (AI) in Bioinformatics Using AI for bioinformatics (e.g. chatbots, GitHub Copilot) can drastically streamline time to project completion as it can be very useful when converting code from one language to another, developing functions, working out bugs, and more. However, it is very important to consider the information and data we enter into a chatbot or AI assistant such as GitHub Copilot. Do not paste any identifiable patient data into chatbots, whether in the form of files or free text. Do not utilize any AI software in conjuction with GitHub or your text editor so that private repositories are not exposed.","title":"Home"},{"location":"#documentation-for-the-rokita-lab-and-bti-bioinformatics-core","text":"Welcome to ReadtheDocs for Children's National Hospital Rokita Lab and Brain Tumor Institute (BTI) Bioinformatics team members. This website is a centralized location that contains information about tool accessibility as well as general guidance for utilizing compute infrastructure and using reproducible practices for analytical projects. To file an issue, please see the docs repo here .","title":"Documentation for the Rokita Lab and BTI Bioinformatics Core"},{"location":"#responsible-and-ethical-use-of-bioinformatics-resources","text":"","title":"Responsible and Ethical Use of Bioinformatics Resources"},{"location":"#ethical-considerations","text":"Please be sure to work with your PI and/or Dr. Rokita to understand which metadata, clinical data, and genomic data types are allowed to be stored in public and private spaces. Please do not share any passwords, tokens, account numbers (e.g. AWS), or IP addresses externally or within GitHub comments, issues, or commits. If you have questions, please ask us!","title":"Ethical Considerations"},{"location":"#submitting-jobs-or-tasks","text":"HPC: When utilizing the HPC, please be mindful when submitting large or many jobs, as this is a community resource. AWS: We have created an EC2 service catalog product, \"BTI Research\" for use by researchers in the BTI. Each person can utlize their own instance without affecting others' compute. CAVATICA : We primarily utilize CAVATICA for established upstream workflows. Jobs will be automatically scheduled and queued based on your user capacity.","title":"Submitting Jobs or Tasks"},{"location":"#fiscal-responsibility","text":"AWS: Please only launch an EC2 instance type and volume for the work you will need to do to minimize costs (We are charged for instances per hour and by storage). Below are some ways you can minimize costs. Always STOP your instance once you are finished, or if are not actively working on the instance (e.g. you have 3 meetings and will not be working on it - you can stop it and come back later). TERMINATE your instance once you no longer need it, or if you will not be using it for an extended period of time (weeks - months). CAVATICA: Use of CAVATICA requires set up of a billing group and funds. Please work with your PI and/or Dr. Rokita if you would like to run tasks in CAVATICA. The NIH has Cloud Credits available for working with specific datasets through application (e.g. CFDE and/or Kids First ). Cancer Genomics Cloud (CGC) : New users of the CGC get $300 in cloud credits when they create a new account, and this can be used to run and/or test new workflows as desired. CGC has cloud credits available through application as well.","title":"Fiscal Responsibility"},{"location":"#responsible-use-of-artificial-intelligence-ai-in-bioinformatics","text":"Using AI for bioinformatics (e.g. chatbots, GitHub Copilot) can drastically streamline time to project completion as it can be very useful when converting code from one language to another, developing functions, working out bugs, and more. However, it is very important to consider the information and data we enter into a chatbot or AI assistant such as GitHub Copilot. Do not paste any identifiable patient data into chatbots, whether in the form of files or free text. Do not utilize any AI software in conjuction with GitHub or your text editor so that private repositories are not exposed.","title":"Responsible Use of Artificial Intelligence (AI) in Bioinformatics"},{"location":"access/","text":"Access to Resources and Tools GitHub We operate two GitHub organizations, the Rokita Lab and BTI Bioinformatics Core . All Rokita Lab and BTI Bioinformatics Core staff should be added to both. Qumulo (L Drive) Requesting access Please create a new Access Request issue and obtain approval from Dr. Rokita. While on Children's National VPN, create an \"Incident type\" IS request in Service Now and request access to smb://cnmc.org/cri/Lab/CancerImmunology-BTI . Mapping the L Drive To map the L drive on a macbook: - While on VPN, Go to the Finder --> Go --> Connect to server and add the path: smb://cnmc.org/cri/Lab/CancerImmunology-BTI . Instructions for depositing data into CancerImmunology-BTI Use this ONLY for raw genomics data which will need to be processed by the BTI Bioinformatics Core and file manifests for those raw data. Please, no word docs, ppts, etc. Create one root folder per lab (eg: FonsecaLab ) Within the lab folder, create project-specific folders. Within project-specific folders, feel free to organize how you'd like (for now \ud83d\ude42). We can now access the data via HPC and/or upload to AWS very quickly. High Performance Cluster (HPC) Requesting access While on VPN, fill out this form for HPC access. HPC basics The HPC Wiki can be found here . You can log into the hpc using ssh hpc.cnmc.org or alternatively to a specific login node using ssh pphpcln01.cnmc.org or ssh pphpcln02.cnmc.org . To view available modules, use module avail . To load modules, use module load xxx To mount L drive folders (e.g. to mount the L Drive folder CancerImmunology-BTI ), follow the instructions below, changing the group to the group to which you belong. #!/bin/bash # Add this file under $HOME/.bashrc.d/ with permission '0640' FOLDER=$HOME/CancerImmunology-BTI CIFS=\"//cnmc.org/CRI_LAB1/CancerImmunology-BTI\" GROUP=\"rokitagrp\" # These settings are generally fine USER=$(whoami) FILE_MODE='0750' DIR_MODE='0750' case $(hostname) in pphpcln*|pphpcdtn* ) if [ ! -d $FOLDER ]; then mkdir -p $FOLDER fi if ! mountpoint -q $FOLDER then echo $FOLDER is not mounted echo Mounting it... sudo /usr/sbin/mount.cifs -o username=${USER},domain=cnmc.org,uid=${USER},gid=${GROUP},dir_mode=${DIR_MODE},file_mode=${FILE_MODE} $CIFS $FOLDER fi esac After sourcing this file, you should be able to see the folder in /home/USER/CancerImmunology-BTI . Transferring external data onto or from the HPC In order to transfer files from Globus to the L drive or vice versa, you must first ssh into the transfer node after completing the above steps: ssh hpc-transfer.cnmc.org This is also the node that you should use for conducting large data transfers from external SFTP servers (for example, from sequencing centers). If an SFTP server is not accessible from the HPC transfer node, you should fill out a \"policy exception\" ticket in service now for IT security to whitelist the server and port. Amazon Web Services (AWS) For information about our account, please see pinned file in Slack. Requesting access First, obtain email approval from Dr. Jo Lynne Rokita for access to the BTI Bioinformatics AWS account. Open a Service Catalog item in Service Now using the \"Add user to existing Access Group for AWS\" template. AWS account name and number = <see slack for account number> Request ReadOnly access Accessing AWS through the console Once access is granted, you should be able to access our account using Children's National SSO credentials using this URL . You will have read access to S3 buckets and EC2 Service Catalog launch permissions. CAVATICA If you do not already have an account, you can generate one using your eCommons ID here . If you do not have an eCommons ID, please work with Dr. Rokita to get one. If you have never received pilot funds when you signed up with CAVATICA, please email support@velsera.com , ask for pilot funds, and they will direct your email to Dr. Jared Rozowsky for $100 in credits. If you have a project in which you will utilize NIH Kids First datasets or workflows, or wish to enhance Kids First workflows, you may apply for Kids First Cloud Credits for CAVATICA, which will get you $1,000 in starter credits. Cancer Genomics Cloud (CGC) CGC is an NIH-hosted CAVATICA instance specifically geared toward cancer researchers and can be accessed through eCommons, as above. When you first log in, you will be awarded $300 in pilot cloud credits. Cancer Genomics Cloud may award up to $10K in cloud credits for bigger project ideas: apply here . Globus Globus is a file transfer service created by University of Chicago. Some sequencing centers may utilize Globus to transfer data to us and they will upload a directory of files to Globus for our retrieval. Additionally, we now have an HPC endpoint called Local FS (HPC@CNRIC) for transfer and since we can map the L drive here, we can essentially directly transfer from Globus to L drive. Each person at Children's National should automatically be able to log into Globus here using Children's National Health System SSO. If you are notified of a project which uses Globus, the user just needs to add your email in order to access the project. You can log into Globus, open the Collection in File Manager , set up the destination to Local FS within your folder of choice (see below), then click on Transfer or Sync to... . The transfer window can be closed and you will received an email when the transfer is completed. Self-service password reset On occasion, your password may be reset without warning (!). To reset it, go to this url and follow the instructions. Box We use Box to store various documents for grants, manuscripts, presentations, and collaborator files. Open a Service Catalog item in Service Now and request access to Box for Children's National. Google Drive We favor Google Drive over OneDrive for collaborative manuscript writing and collaborative documentation. Currently, Google Drive is sometimes (or always?) blocked on PCs, but to date, it is available on Apple devices. Paperpile We have paperpile licenses for individuals actively writing manuscripts on Google Drive needing to insert references. There is now a beta version of a Microsoft Word plug-in for paperpile, which can be downloaded here . Instructions for how to use the Word plug-in can be found here . Please ask Dr. Rokita for a license if you are contributing to grants and/or manuscripts. Note: Paperpile \"format citations\" has a scripting error while on VPN, so you may need to disconnet from Children's National VPN and toggle to GWU VPN for full functionality. GWU ID and GWU HPC Access Fill out the RedCap form here , which will trigger an email to Adam Wong, who will take care of the request.","title":"Technology Access Requests"},{"location":"access/#access-to-resources-and-tools","text":"","title":"Access to Resources and Tools"},{"location":"access/#github","text":"We operate two GitHub organizations, the Rokita Lab and BTI Bioinformatics Core . All Rokita Lab and BTI Bioinformatics Core staff should be added to both.","title":"GitHub"},{"location":"access/#qumulo-l-drive","text":"","title":"Qumulo (L Drive)"},{"location":"access/#requesting-access","text":"Please create a new Access Request issue and obtain approval from Dr. Rokita. While on Children's National VPN, create an \"Incident type\" IS request in Service Now and request access to smb://cnmc.org/cri/Lab/CancerImmunology-BTI .","title":"Requesting access"},{"location":"access/#mapping-the-l-drive","text":"To map the L drive on a macbook: - While on VPN, Go to the Finder --> Go --> Connect to server and add the path: smb://cnmc.org/cri/Lab/CancerImmunology-BTI .","title":"Mapping the L Drive"},{"location":"access/#instructions-for-depositing-data-into-cancerimmunology-bti","text":"Use this ONLY for raw genomics data which will need to be processed by the BTI Bioinformatics Core and file manifests for those raw data. Please, no word docs, ppts, etc. Create one root folder per lab (eg: FonsecaLab ) Within the lab folder, create project-specific folders. Within project-specific folders, feel free to organize how you'd like (for now \ud83d\ude42). We can now access the data via HPC and/or upload to AWS very quickly.","title":"Instructions for depositing data into CancerImmunology-BTI"},{"location":"access/#high-performance-cluster-hpc","text":"","title":"High Performance Cluster (HPC)"},{"location":"access/#requesting-access_1","text":"While on VPN, fill out this form for HPC access.","title":"Requesting access"},{"location":"access/#hpc-basics","text":"The HPC Wiki can be found here . You can log into the hpc using ssh hpc.cnmc.org or alternatively to a specific login node using ssh pphpcln01.cnmc.org or ssh pphpcln02.cnmc.org . To view available modules, use module avail . To load modules, use module load xxx To mount L drive folders (e.g. to mount the L Drive folder CancerImmunology-BTI ), follow the instructions below, changing the group to the group to which you belong. #!/bin/bash # Add this file under $HOME/.bashrc.d/ with permission '0640' FOLDER=$HOME/CancerImmunology-BTI CIFS=\"//cnmc.org/CRI_LAB1/CancerImmunology-BTI\" GROUP=\"rokitagrp\" # These settings are generally fine USER=$(whoami) FILE_MODE='0750' DIR_MODE='0750' case $(hostname) in pphpcln*|pphpcdtn* ) if [ ! -d $FOLDER ]; then mkdir -p $FOLDER fi if ! mountpoint -q $FOLDER then echo $FOLDER is not mounted echo Mounting it... sudo /usr/sbin/mount.cifs -o username=${USER},domain=cnmc.org,uid=${USER},gid=${GROUP},dir_mode=${DIR_MODE},file_mode=${FILE_MODE} $CIFS $FOLDER fi esac After sourcing this file, you should be able to see the folder in /home/USER/CancerImmunology-BTI .","title":"HPC basics"},{"location":"access/#transferring-external-data-onto-or-from-the-hpc","text":"In order to transfer files from Globus to the L drive or vice versa, you must first ssh into the transfer node after completing the above steps: ssh hpc-transfer.cnmc.org This is also the node that you should use for conducting large data transfers from external SFTP servers (for example, from sequencing centers). If an SFTP server is not accessible from the HPC transfer node, you should fill out a \"policy exception\" ticket in service now for IT security to whitelist the server and port.","title":"Transferring external data onto or from the HPC"},{"location":"access/#amazon-web-services-aws","text":"For information about our account, please see pinned file in Slack.","title":"Amazon Web Services (AWS)"},{"location":"access/#requesting-access_2","text":"First, obtain email approval from Dr. Jo Lynne Rokita for access to the BTI Bioinformatics AWS account. Open a Service Catalog item in Service Now using the \"Add user to existing Access Group for AWS\" template. AWS account name and number = <see slack for account number> Request ReadOnly access","title":"Requesting access"},{"location":"access/#accessing-aws-through-the-console","text":"Once access is granted, you should be able to access our account using Children's National SSO credentials using this URL . You will have read access to S3 buckets and EC2 Service Catalog launch permissions.","title":"Accessing AWS through the console"},{"location":"access/#cavatica","text":"If you do not already have an account, you can generate one using your eCommons ID here . If you do not have an eCommons ID, please work with Dr. Rokita to get one. If you have never received pilot funds when you signed up with CAVATICA, please email support@velsera.com , ask for pilot funds, and they will direct your email to Dr. Jared Rozowsky for $100 in credits. If you have a project in which you will utilize NIH Kids First datasets or workflows, or wish to enhance Kids First workflows, you may apply for Kids First Cloud Credits for CAVATICA, which will get you $1,000 in starter credits.","title":"CAVATICA"},{"location":"access/#cancer-genomics-cloud-cgc","text":"CGC is an NIH-hosted CAVATICA instance specifically geared toward cancer researchers and can be accessed through eCommons, as above. When you first log in, you will be awarded $300 in pilot cloud credits. Cancer Genomics Cloud may award up to $10K in cloud credits for bigger project ideas: apply here .","title":"Cancer Genomics Cloud (CGC)"},{"location":"access/#globus","text":"Globus is a file transfer service created by University of Chicago. Some sequencing centers may utilize Globus to transfer data to us and they will upload a directory of files to Globus for our retrieval. Additionally, we now have an HPC endpoint called Local FS (HPC@CNRIC) for transfer and since we can map the L drive here, we can essentially directly transfer from Globus to L drive. Each person at Children's National should automatically be able to log into Globus here using Children's National Health System SSO. If you are notified of a project which uses Globus, the user just needs to add your email in order to access the project. You can log into Globus, open the Collection in File Manager , set up the destination to Local FS within your folder of choice (see below), then click on Transfer or Sync to... . The transfer window can be closed and you will received an email when the transfer is completed.","title":"Globus"},{"location":"access/#self-service-password-reset","text":"On occasion, your password may be reset without warning (!). To reset it, go to this url and follow the instructions.","title":"Self-service password reset"},{"location":"access/#box","text":"We use Box to store various documents for grants, manuscripts, presentations, and collaborator files. Open a Service Catalog item in Service Now and request access to Box for Children's National.","title":"Box"},{"location":"access/#google-drive","text":"We favor Google Drive over OneDrive for collaborative manuscript writing and collaborative documentation. Currently, Google Drive is sometimes (or always?) blocked on PCs, but to date, it is available on Apple devices.","title":"Google Drive"},{"location":"access/#paperpile","text":"We have paperpile licenses for individuals actively writing manuscripts on Google Drive needing to insert references. There is now a beta version of a Microsoft Word plug-in for paperpile, which can be downloaded here . Instructions for how to use the Word plug-in can be found here . Please ask Dr. Rokita for a license if you are contributing to grants and/or manuscripts. Note: Paperpile \"format citations\" has a scripting error while on VPN, so you may need to disconnet from Children's National VPN and toggle to GWU VPN for full functionality.","title":"Paperpile"},{"location":"access/#gwu-id-and-gwu-hpc-access","text":"Fill out the RedCap form here , which will trigger an email to Adam Wong, who will take care of the request.","title":"GWU ID and GWU HPC Access"},{"location":"aws/","text":"AWS Computing Setting up AWS Single-sign-on (SSO) Make sure you have AWS cli installed on your local machine. aws configure sso SSO session name (Recommended): <anything> SSO start URL [None]: <add the start URL, can be found in slack> SSO region [None]: us-east-1 <Enter> This should open a URL in your browser - accept access CLI default client Region [None]: us-east-1 CLI default output format [None]: CLI profile name [XXX-AccountNumber]: cnh-sso You can now use this new single-sign on profile as aws s3 ls --profile cnh-sso Launching an EC2 Instance from AWS Service Catalog We have created an EC2 instance template with precompiled utilities and tools, available within the AWS Service Catalog . Navigate to the Service Catalog . Under Provisioning , select Products . Select BTI Research and Click Launch Product . Enter a name for this product so you'll easily remember why you created it (e.g. \"Rokita-code-reviews\" or \"Rokita-dev\"). Follow the remaining instructions, filling in your GitHub email and username. Select an instance type (specs below): Select the amount of storage you'd like, keeping in mind that this can be expanded later and we pay for the storage we select. Typically, you can start at 500 GB or 1 TB. Click Launch product . At this point, the instance will start creation and once complete, you will be able to see the instance ID ( i-################### ). Click the instance ID link and navigate to the top right of the page, where you will see the Private IPv4 addresses . While on VPN, copy the IP address, and in your local terminal, type ssh ubuntu@ip_address . You are now connected to your EC2 instance! To begin working with GitHub on the EC2 instance, you must create an SSH key on the instance and add it to GitHub (see GitHub section). Stopping or Terminating an EC2 Instance To STOP an EC2 instance (or shut it down for later), you can either: Type sudo shutdown -h now into your console while inside of the instance OR Navigate to the instance within the AWS Console, click on Instance state , and Click Stop instance , as below: To TERMINATE an EC2 instance (or delete it for good and stop storage charges on it), navigate to the instance within the AWS Console, click on Instance state , and Click Terminate (delete) instance .","title":"AWS Compute"},{"location":"aws/#aws-computing","text":"","title":"AWS Computing"},{"location":"aws/#setting-up-aws-single-sign-on-sso","text":"Make sure you have AWS cli installed on your local machine. aws configure sso SSO session name (Recommended): <anything> SSO start URL [None]: <add the start URL, can be found in slack> SSO region [None]: us-east-1 <Enter> This should open a URL in your browser - accept access CLI default client Region [None]: us-east-1 CLI default output format [None]: CLI profile name [XXX-AccountNumber]: cnh-sso You can now use this new single-sign on profile as aws s3 ls --profile cnh-sso","title":"Setting up AWS Single-sign-on (SSO)"},{"location":"aws/#launching-an-ec2-instance-from-aws-service-catalog","text":"We have created an EC2 instance template with precompiled utilities and tools, available within the AWS Service Catalog . Navigate to the Service Catalog . Under Provisioning , select Products . Select BTI Research and Click Launch Product . Enter a name for this product so you'll easily remember why you created it (e.g. \"Rokita-code-reviews\" or \"Rokita-dev\"). Follow the remaining instructions, filling in your GitHub email and username. Select an instance type (specs below): Select the amount of storage you'd like, keeping in mind that this can be expanded later and we pay for the storage we select. Typically, you can start at 500 GB or 1 TB. Click Launch product . At this point, the instance will start creation and once complete, you will be able to see the instance ID ( i-################### ). Click the instance ID link and navigate to the top right of the page, where you will see the Private IPv4 addresses . While on VPN, copy the IP address, and in your local terminal, type ssh ubuntu@ip_address . You are now connected to your EC2 instance! To begin working with GitHub on the EC2 instance, you must create an SSH key on the instance and add it to GitHub (see GitHub section).","title":"Launching an EC2 Instance from AWS Service Catalog"},{"location":"aws/#stopping-or-terminating-an-ec2-instance","text":"To STOP an EC2 instance (or shut it down for later), you can either: Type sudo shutdown -h now into your console while inside of the instance OR Navigate to the instance within the AWS Console, click on Instance state , and Click Stop instance , as below: To TERMINATE an EC2 instance (or delete it for good and stop storage charges on it), navigate to the instance within the AWS Console, click on Instance state , and Click Terminate (delete) instance .","title":"Stopping or Terminating an EC2 Instance"},{"location":"cavatica/","text":"CAVATICA Computing CAVATICA is a platform which enables users to run AWS compute jobs through Common Workflow Language (CWL). As such, we have to pay for analyses run on CAVATICA and storage of data on CAVATICA which is not mounted to any of our AWS S3 buckets. Keeping this in mind, we will discuss as a team what can be run when and with what money. We will generally use CAVATICA to: Run upstream processing workflows on raw data files, for example, those created through the Kids First Data Resource Center (KF DRC) at CHOP. Benchmark and implement new workflows specific to the needs of the BTI, for example, CITE-Seq, miRNA-Seq, pangenome analyses, etc. Small-scale merges/quick analyses of many data files to avoid downloads, such as through Data Studio. How to mount an AWS volume in CAVATICA In order to mount an AWS volume, you need to have access keys to our AWS account. We will not give these out to everyone, so generally, Alex Sickler and Jo Lynne Rokita will be the ones with keys for mounting. Lordley Okarter, who is the AWS account admin, is the only one who can create these. Once you have keys: Go to Data \u2014> Volumes Follow the instructions for the bucket you would like to mount Copy the IAM policy, save it as a JSON file, and add it via PR to the BTI bucket policy repository . Then, send this JSON file to Lordley Okarter (cc Jo Lynne), as he is currently the only person who can add these to the IAM policy for our AWS account. Once Lordley confirms this was added, you can proceed. Add your access keys. For FIPS, use s3-fips.us-east-1.amazonaws.com For encryption, use default 256 . Note: we usually will mount these as read/write if we desire to export data back to the bucket. Additional documentation on mounting volumes inside of CAVATICA can be found here .","title":"CAVATICA Compute"},{"location":"cavatica/#cavatica-computing","text":"CAVATICA is a platform which enables users to run AWS compute jobs through Common Workflow Language (CWL). As such, we have to pay for analyses run on CAVATICA and storage of data on CAVATICA which is not mounted to any of our AWS S3 buckets. Keeping this in mind, we will discuss as a team what can be run when and with what money. We will generally use CAVATICA to: Run upstream processing workflows on raw data files, for example, those created through the Kids First Data Resource Center (KF DRC) at CHOP. Benchmark and implement new workflows specific to the needs of the BTI, for example, CITE-Seq, miRNA-Seq, pangenome analyses, etc. Small-scale merges/quick analyses of many data files to avoid downloads, such as through Data Studio.","title":"CAVATICA Computing"},{"location":"cavatica/#how-to-mount-an-aws-volume-in-cavatica","text":"In order to mount an AWS volume, you need to have access keys to our AWS account. We will not give these out to everyone, so generally, Alex Sickler and Jo Lynne Rokita will be the ones with keys for mounting. Lordley Okarter, who is the AWS account admin, is the only one who can create these. Once you have keys: Go to Data \u2014> Volumes Follow the instructions for the bucket you would like to mount Copy the IAM policy, save it as a JSON file, and add it via PR to the BTI bucket policy repository . Then, send this JSON file to Lordley Okarter (cc Jo Lynne), as he is currently the only person who can add these to the IAM policy for our AWS account. Once Lordley confirms this was added, you can proceed. Add your access keys. For FIPS, use s3-fips.us-east-1.amazonaws.com For encryption, use default 256 . Note: we usually will mount these as read/write if we desire to export data back to the bucket. Additional documentation on mounting volumes inside of CAVATICA can be found here .","title":"How to mount an AWS volume in CAVATICA"},{"location":"cnh-onboarding/","text":"Children's National-specific onboarding Virtual Private Networks (VPNs) Children's National Hospital (Children's National) Children's National has recently switched their VPN to Palo Alto GlobalProtect, is installed on your computer. To connect, enter the Children's National portal and you will be asked to enter your credentials. For remote users, GlobalProtect will be on \"always on\" mode. For more information, see the intranet posting here . George Washington University (GWU) GWU also uses GlobalProtect. To request a GWU ID, please refer to the access request section . To connect, enter the GWU portal gwvpn.gwu.edu and you will be asked to enter your credentials. Exception to \"Always On\" VPN To request to be added to the Active Directory (AD) group for GlobalProtect, allowing VPN disconnect, fill out the RedCap form here and email Claudio Anselmi to ask him to create an IS Request for Policy Exception for your user. Once approved, you should be added to the AD group and can call the Help Desk for the password. Children's National Intranet The Children's National Intranet (MS Sharepoint site) can be found here and is relatively easy to navigate. The administrative team within the Center for Cancer and Immunology Research (CCIR) has done a wonderful job creating a CCIR-specific Sharepoint site for its faculty and staff. Training modules As a staff member who will have access to large amounts of genomic data and potentially protected health information (PHI) for patients, we require everyone complete multiple general staff and specific biomedical research traing modules. CITI If you do not have an account set up with CITI, please make one here . If your account has an old email associated, please update to use your Children's National email address and/or add your affiliation to Children's National Medical Center (CNMC). This way, other staff members can search for you and see whether you had completed certain trainings. Next, you should be able to see a number of courses for CNMC. Please complete the following: Biomedical Research (17 modules) RCR: BIOMEDICAL RESEARCH (6 modules) Conflict of Interest (4 modules) Cornerstone Cornerstone is the place where you will complete Children's National-related training/learning modules. From the Children's National Sharepoint , navigate to Resources --> Apps & Tools --> Cornerstone You will periodically see modules assigned. Please complete these before their due dates. Your Timecard Bear Time (Kronos) is the system used for recording time. From the Children's National Sharepoint , navigate to Resources --> Apps & Tools --> Bear Time (Kronos) During your first week, please email Dawn Griffiths and Erica Cleary to set your weekly schedule. They manage all timecards for the Center for Cancer and Immunology Research (CCIR). Once this is completed, you do not need to fill out your timecard weekly, nor approve it. Once you are eligible to take time off (ASSLA after 90 days and Vacation after 90 days (hourly employees) or 180 days (salaried employees)), please follow the instructions below to make a request: 1. Email Dr. Rokita, cc-ing Dawn Griffiths and Erica Cleary, to make a request for time off. 2. Once approved, log into Kronos and make an official request for time off within the right-hand screen. For more information about accruing leave, please see here . BTI Bioinformatics Outlook Calendar We have created a shared calendar in outlook called BTI Bioinformatics . Please place your out of office time on this calendar for team awareness.","title":"CNH-Specific Onboarding"},{"location":"cnh-onboarding/#childrens-national-specific-onboarding","text":"","title":"Children's National-specific onboarding"},{"location":"cnh-onboarding/#virtual-private-networks-vpns","text":"","title":"Virtual Private Networks (VPNs)"},{"location":"cnh-onboarding/#childrens-national-hospital-childrens-national","text":"Children's National has recently switched their VPN to Palo Alto GlobalProtect, is installed on your computer. To connect, enter the Children's National portal and you will be asked to enter your credentials. For remote users, GlobalProtect will be on \"always on\" mode. For more information, see the intranet posting here .","title":"Children's National Hospital (Children's National)"},{"location":"cnh-onboarding/#george-washington-university-gwu","text":"GWU also uses GlobalProtect. To request a GWU ID, please refer to the access request section . To connect, enter the GWU portal gwvpn.gwu.edu and you will be asked to enter your credentials.","title":"George Washington University (GWU)"},{"location":"cnh-onboarding/#exception-to-always-on-vpn","text":"To request to be added to the Active Directory (AD) group for GlobalProtect, allowing VPN disconnect, fill out the RedCap form here and email Claudio Anselmi to ask him to create an IS Request for Policy Exception for your user. Once approved, you should be added to the AD group and can call the Help Desk for the password.","title":"Exception to \"Always On\" VPN"},{"location":"cnh-onboarding/#childrens-national-intranet","text":"The Children's National Intranet (MS Sharepoint site) can be found here and is relatively easy to navigate. The administrative team within the Center for Cancer and Immunology Research (CCIR) has done a wonderful job creating a CCIR-specific Sharepoint site for its faculty and staff.","title":"Children's National Intranet"},{"location":"cnh-onboarding/#training-modules","text":"As a staff member who will have access to large amounts of genomic data and potentially protected health information (PHI) for patients, we require everyone complete multiple general staff and specific biomedical research traing modules.","title":"Training modules"},{"location":"cnh-onboarding/#citi","text":"If you do not have an account set up with CITI, please make one here . If your account has an old email associated, please update to use your Children's National email address and/or add your affiliation to Children's National Medical Center (CNMC). This way, other staff members can search for you and see whether you had completed certain trainings. Next, you should be able to see a number of courses for CNMC. Please complete the following: Biomedical Research (17 modules) RCR: BIOMEDICAL RESEARCH (6 modules) Conflict of Interest (4 modules)","title":"CITI"},{"location":"cnh-onboarding/#cornerstone","text":"Cornerstone is the place where you will complete Children's National-related training/learning modules. From the Children's National Sharepoint , navigate to Resources --> Apps & Tools --> Cornerstone You will periodically see modules assigned. Please complete these before their due dates.","title":"Cornerstone"},{"location":"cnh-onboarding/#your-timecard","text":"Bear Time (Kronos) is the system used for recording time. From the Children's National Sharepoint , navigate to Resources --> Apps & Tools --> Bear Time (Kronos) During your first week, please email Dawn Griffiths and Erica Cleary to set your weekly schedule. They manage all timecards for the Center for Cancer and Immunology Research (CCIR). Once this is completed, you do not need to fill out your timecard weekly, nor approve it. Once you are eligible to take time off (ASSLA after 90 days and Vacation after 90 days (hourly employees) or 180 days (salaried employees)), please follow the instructions below to make a request: 1. Email Dr. Rokita, cc-ing Dawn Griffiths and Erica Cleary, to make a request for time off. 2. Once approved, log into Kronos and make an official request for time off within the right-hand screen. For more information about accruing leave, please see here .","title":"Your Timecard"},{"location":"cnh-onboarding/#bti-bioinformatics-outlook-calendar","text":"We have created a shared calendar in outlook called BTI Bioinformatics . Please place your out of office time on this calendar for team awareness.","title":"BTI Bioinformatics Outlook Calendar"},{"location":"code-review-guide/","text":"Code Review Guidelines All pull requests will undergo peer review, and we do so while maintaining a culture of positive peer review. Our Code Review model, based on OpenPedCan . The review intends to ensure that a pull request conforms to the following basic requirements: The code is correct. The results can be reproduced in a reasonable amount of time. The results are expected. The documentation describes the purpose, methods, results, input, output, and how to run the code. The code follows basic style guidelines. The code contains comments that explain non-obvious procedures. The analysis is scientifically sound. But, why? To foster a culture of collaboration. To allow junior team members to learn from senior team members , or frequently, vice versa! To spark knowledge transfer, not only of coding practices, but also through project-specific analyses and subject matter. To track our changes and thoughts over time (i.e. this is our virtual lab notebook). To maintain our high standard of reproducibility and rigor. To have all code, figures, and tables ready for future manuscript submission. Rules of thumb: Always rerun the module to ensure it works and outputs can be reproduced identically. Review \u2264 400 lines of code (LOC) at a time Do not review for more than 60 minutes at a time. Take 20 minute breaks in between reviews. Pull requests can be reviewed using GitHub's review interface . Following are the basic guidelines for reviewing pull requests: Note the type of review you performed: did you look over the source code, did you look over the documentation, did you run the source code, did you look at and interpret the results or a combination of these? Suggest modifications or, potentially, directly suggest, but do not commit changes to , the pull request. Explain in detail whether and why the suggested modifications are necessary , and how the modifications should be implemented specifically, so that the developers are able to follow the suggestions without misunderstanding. For any suggestions/comments that are related to the lines that were changed in the particular PR, make the comment underneath the code by clicking the \"+\" next to the code (see below images). Click \"Start a review\" for the first comment, and afterwards, click \"Add single comment\" to keep all comments within the same review. Click \"Finish your review\" once completed. If there are any comments/suggestions that are outside of the code changes this particular PR, for example, any questions about the results, or any lines of code (although not changed) that seemed puzzling, mention them as comments after you click \"Finish your review\". The permalinks of the referenced results or code can be obtained by the following procedure: click the \"Commits\" tab of the pull request, browse the repository at a specific commit that contains the referenced results or code by clicking the <> button of the commit row, go to the referenced file in the repository to get file or line permalinks. If the suggested modifications are extensive refactoring or re-designing without any change on the code behaviors, results or run-time, consider submitting a new issue for the refactoring or re-designing, and discussing the priority and specific implementations in the new issue, after the pull request under review is merged, so that the pull request under review will not be kept open for lengthy discussions and commits on extensive refactoring or re-designing that are out of the scope of the original pull request addressed issue. Before a repository maintainer merges a pull request, there must be at least one affirmative review. If there is any unaddressed criticism or disapproval, a repository maintainer will determine how to proceed and may wait for additional feedback. Merging approved pull requests If working on a collaborative repository, and a pull request is approved, please do not merge it immediately. Instead, wait for the repository maintainer to coordinate with you on when to merge the pull request and whether the pull request should be updated with the latest dev or main branch. The coordination can reduce the workload of developers for checking whether the dev or main branch updates by merging other pull requests will affect their own pull requests. Additional Code Review Resources Fred Hutch Data Science Lab SmartBear Code Review ALSF GitHub and Code Review Workshop The Childhood Cancer Data Lab develops tools and training programs to empower childhood cancer researchers to utilize data to make more robust discoveries. The Childhood Cancer Data Lab is an initiative of Alex's Lemonade Stand Foundation . This workshop will introduce concepts in version control and project management with git and GitHub, with a focus on analytical code from the perspective of both code authors and reviewers. The material in this workshop will assume participants have some practical experience using git and GitHub. You may wish you review the git resources we\u2019ve compiled to help you get started. Materials: Website Slides Recordings: 01 Intro to Git, Part 1 02 Intro to Git, Part 2 03 Intro to analytical code review, Part 1 04 Intro to analytical code review, Part 2 (1/2) 05 Intro to analytical code review, Part 2 (2/2)","title":"Code Review Guidelines"},{"location":"code-review-guide/#code-review-guidelines","text":"All pull requests will undergo peer review, and we do so while maintaining a culture of positive peer review.","title":"Code Review Guidelines"},{"location":"code-review-guide/#our-code-review-model-based-on-openpedcan","text":"The review intends to ensure that a pull request conforms to the following basic requirements: The code is correct. The results can be reproduced in a reasonable amount of time. The results are expected. The documentation describes the purpose, methods, results, input, output, and how to run the code. The code follows basic style guidelines. The code contains comments that explain non-obvious procedures. The analysis is scientifically sound. But, why? To foster a culture of collaboration. To allow junior team members to learn from senior team members , or frequently, vice versa! To spark knowledge transfer, not only of coding practices, but also through project-specific analyses and subject matter. To track our changes and thoughts over time (i.e. this is our virtual lab notebook). To maintain our high standard of reproducibility and rigor. To have all code, figures, and tables ready for future manuscript submission.","title":"Our Code Review model, based on OpenPedCan."},{"location":"code-review-guide/#rules-of-thumb","text":"Always rerun the module to ensure it works and outputs can be reproduced identically. Review \u2264 400 lines of code (LOC) at a time Do not review for more than 60 minutes at a time. Take 20 minute breaks in between reviews. Pull requests can be reviewed using GitHub's review interface . Following are the basic guidelines for reviewing pull requests: Note the type of review you performed: did you look over the source code, did you look over the documentation, did you run the source code, did you look at and interpret the results or a combination of these? Suggest modifications or, potentially, directly suggest, but do not commit changes to , the pull request. Explain in detail whether and why the suggested modifications are necessary , and how the modifications should be implemented specifically, so that the developers are able to follow the suggestions without misunderstanding. For any suggestions/comments that are related to the lines that were changed in the particular PR, make the comment underneath the code by clicking the \"+\" next to the code (see below images). Click \"Start a review\" for the first comment, and afterwards, click \"Add single comment\" to keep all comments within the same review. Click \"Finish your review\" once completed. If there are any comments/suggestions that are outside of the code changes this particular PR, for example, any questions about the results, or any lines of code (although not changed) that seemed puzzling, mention them as comments after you click \"Finish your review\". The permalinks of the referenced results or code can be obtained by the following procedure: click the \"Commits\" tab of the pull request, browse the repository at a specific commit that contains the referenced results or code by clicking the <> button of the commit row, go to the referenced file in the repository to get file or line permalinks. If the suggested modifications are extensive refactoring or re-designing without any change on the code behaviors, results or run-time, consider submitting a new issue for the refactoring or re-designing, and discussing the priority and specific implementations in the new issue, after the pull request under review is merged, so that the pull request under review will not be kept open for lengthy discussions and commits on extensive refactoring or re-designing that are out of the scope of the original pull request addressed issue. Before a repository maintainer merges a pull request, there must be at least one affirmative review. If there is any unaddressed criticism or disapproval, a repository maintainer will determine how to proceed and may wait for additional feedback.","title":"Rules of thumb:"},{"location":"code-review-guide/#merging-approved-pull-requests","text":"If working on a collaborative repository, and a pull request is approved, please do not merge it immediately. Instead, wait for the repository maintainer to coordinate with you on when to merge the pull request and whether the pull request should be updated with the latest dev or main branch. The coordination can reduce the workload of developers for checking whether the dev or main branch updates by merging other pull requests will affect their own pull requests.","title":"Merging approved pull requests"},{"location":"code-review-guide/#additional-code-review-resources","text":"Fred Hutch Data Science Lab SmartBear Code Review","title":"Additional Code Review Resources"},{"location":"code-review-guide/#alsf-github-and-code-review-workshop","text":"The Childhood Cancer Data Lab develops tools and training programs to empower childhood cancer researchers to utilize data to make more robust discoveries. The Childhood Cancer Data Lab is an initiative of Alex's Lemonade Stand Foundation . This workshop will introduce concepts in version control and project management with git and GitHub, with a focus on analytical code from the perspective of both code authors and reviewers. The material in this workshop will assume participants have some practical experience using git and GitHub. You may wish you review the git resources we\u2019ve compiled to help you get started. Materials: Website Slides Recordings: 01 Intro to Git, Part 1 02 Intro to Git, Part 2 03 Intro to analytical code review, Part 1 04 Intro to analytical code review, Part 2 (1/2) 05 Intro to analytical code review, Part 2 (2/2)","title":"ALSF GitHub and Code Review Workshop"},{"location":"docker/","text":"Using Docker or Podman for Containerizing Code We will containerize all packages which can be redistributed without licenses using Docker containers. You can work with these using either Docker or Podman . Creating a New Dockerfile and Docker Image Create a local Dockerfile that loads/installs necessary packages (and their versions) necessary to run the code in the Github repo. How to create a docker registry in CAVATICA Create a repository Log into CAVATICA using one of the lab accounts. CAVATICA logins for Children's BTI and Rokita Lab are pinned to #rokita-lab-internal slack channel. Click on Developer tab -> Docker registry Click + Create repository (top right) Type repository name and choose visibility and click create . Assign admin rights to Jo Lynne Rokita ( harenzaj ) and Alex Sickler ( sicklera ) by clicking on: <Name of Repository>/ Members Assign admin rights to yourself so that you can push the image once it's built Docker Login From the directory where your Dockerfile lives, log into CAVATICA docker registry using your CAVATICA credentials. docker login http://pgc-images.sbgenomics.com/ -u <USERNAME> -p <YOUR-AUTH-TOKEN> \"USERNAME\" refers to your CAVATICA username. \"YOUR-AUTH-TOKEN\" can be generated from your CAVATICA account under the \u201cAuthentication token\u201d section in the Developer tab. Build Image docker build -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest . For Mac users with Apple silicon chips (M1-M4 chip), you may need to specify the platform docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest . Push the Docker Image docker push pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] where <username>/<repository_name>[:tag] refers to the image that you have already committed Example : docker push pgc-images.sbgenomics.com/naqvia/autopvs1:latest Pulling the Docker Image docker pull pgc-images.sbgenomics.com/<username>/<repository_name>[:tag]:<tagname> Example : docker pull pgc-images.sbgenomics.com/naqvia/autopvs1:latest Updating the Docker Image Make any necessary changes to Dockerfile and run docker build as done previously. Example : docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/<username>/<repository_name>:latest . Here, \"username\" should match the namespace used in the existing Docker container. For example, it should be \"rokita-lab\", not your personal CAVATICA username, in the repository below. Once complete, the output will print Successfully built <image ID> . Tag this image ID to the remote repo and push using these steps: docker tag <image ID> pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] docker push pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] For more, please review the full documentation . For more about Docker using M1/M2/M3 chip macs, ( see here ).","title":"Using Docker or Podman"},{"location":"docker/#using-docker-or-podman-for-containerizing-code","text":"We will containerize all packages which can be redistributed without licenses using Docker containers. You can work with these using either Docker or Podman .","title":"Using Docker or Podman for Containerizing Code"},{"location":"docker/#creating-a-new-dockerfile-and-docker-image","text":"Create a local Dockerfile that loads/installs necessary packages (and their versions) necessary to run the code in the Github repo. How to create a docker registry in CAVATICA Create a repository Log into CAVATICA using one of the lab accounts. CAVATICA logins for Children's BTI and Rokita Lab are pinned to #rokita-lab-internal slack channel. Click on Developer tab -> Docker registry Click + Create repository (top right) Type repository name and choose visibility and click create . Assign admin rights to Jo Lynne Rokita ( harenzaj ) and Alex Sickler ( sicklera ) by clicking on: <Name of Repository>/ Members Assign admin rights to yourself so that you can push the image once it's built Docker Login From the directory where your Dockerfile lives, log into CAVATICA docker registry using your CAVATICA credentials. docker login http://pgc-images.sbgenomics.com/ -u <USERNAME> -p <YOUR-AUTH-TOKEN> \"USERNAME\" refers to your CAVATICA username. \"YOUR-AUTH-TOKEN\" can be generated from your CAVATICA account under the \u201cAuthentication token\u201d section in the Developer tab. Build Image docker build -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest . For Mac users with Apple silicon chips (M1-M4 chip), you may need to specify the platform docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest . Push the Docker Image docker push pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] where <username>/<repository_name>[:tag] refers to the image that you have already committed Example : docker push pgc-images.sbgenomics.com/naqvia/autopvs1:latest","title":"Creating a New Dockerfile and Docker Image"},{"location":"docker/#pulling-the-docker-image","text":"docker pull pgc-images.sbgenomics.com/<username>/<repository_name>[:tag]:<tagname> Example : docker pull pgc-images.sbgenomics.com/naqvia/autopvs1:latest","title":"Pulling the Docker Image"},{"location":"docker/#updating-the-docker-image","text":"Make any necessary changes to Dockerfile and run docker build as done previously. Example : docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/<username>/<repository_name>:latest . Here, \"username\" should match the namespace used in the existing Docker container. For example, it should be \"rokita-lab\", not your personal CAVATICA username, in the repository below. Once complete, the output will print Successfully built <image ID> . Tag this image ID to the remote repo and push using these steps: docker tag <image ID> pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] docker push pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] For more, please review the full documentation . For more about Docker using M1/M2/M3 chip macs, ( see here ).","title":"Updating the Docker Image"},{"location":"docs-resources/","text":"How to update documentation This documentation was created with mkdocs.org . To update, please submit a PR to GitHub . Commands (automated through GitHub Actions) mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Docs"},{"location":"docs-resources/#how-to-update-documentation","text":"This documentation was created with mkdocs.org . To update, please submit a PR to GitHub .","title":"How to update documentation"},{"location":"docs-resources/#commands-automated-through-github-actions","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands (automated through GitHub Actions)"},{"location":"docs-resources/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"github-general/","text":"Using GitHub For general information on using GitHub, see here . Setting up a local SSH Key and adding to GitHub On your local computer (or EC2 instance for the first time), run the following command to generate your public SSH key. ssh-keygen <enter> <enter> <enter> cat ~/.ssh/id_rsa.pub Add this new SSH key to your profile in GitHub by following these instructions . Setting up a GitHub repository Clone remote GitHub repo onto local computer using SSH: git clone git@github.com:rokitalab/OpenPedCan-Project-CNH.git Obtain latest version of project Docker image using Docker or Podman: docker pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest podman pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Run the Docker container For Local Development : docker run --name <CONTAINER_NAME> --platform linux/amd64 -d -e PASSWORD=pass -p 8787:8787 -v $PWD:/home/rstudio/OpenPedCan-Project-CNH pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Alternatively, the container can be initialized through the Docker dashboard on desktop. Mac and Linux users can also run Rstudio in the project docker container from a web browser. After executing the above docker run command, navigate to localhost:8787 in your web browser. The username for login is rstudio and the password will be whatever password is set in the docker run command above (default: pass ) For development using Amazon EC2 : docker run --platform linux/amd64 --name <CONTAINER_NAME> -d -e PASSWORD=pass -p 80:8787 -v $PWD:/home/rstudio/OpenPedCan-Project-CNH pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest To launch RStudio in a browser, enter the IP address in a web browser. The username for login is rstudio and the password is pass (default) or whatever was specified in the docker run command. (Optional) Once running, a bash shell can be opened into the container to run analyses. For example if <CONTAINER NAME> = openpedcan, run the following command from the root directory: docker exec -ti openpedcan bash From here, you can navigate to the module of interest as follows: cd /home/rstudio/OpenPedCan-Project-CNH/analyses/module-of-interest Download project data Data can typically be downloaded via a download-data.sh shell script in the project root directory: bash download-data.sh This will download all project data from the associated Amazon S3 bucket and create symlinks in data/ to the latest data release version. This command can be re-run to ensure that the most updated files are downloaded. Data release instructions For generating a new versioned data release, follow the steps below: Create a release-notes.md file This file should document the contents of the release, including: - Version name (e.g., v1) - Release date - Description of each data file - Any relevant changelog-style notes Use the format from the OpenPedCan project as a reference: OpenPedCan release-notes.md example Place a copy of release-notes.md in the GitHub repo under: doc/release-notes.md Create a versioned folder in S3 Create the new version folder in the designated S3 bucket: s3://<bucket-name>/<repo-name>/v1/ Add all finalized data files for this release into the versioned folder. Use --dryrun to test the copy from local to S3, and then remove it once you are sure the copy will go to the correct location. aws s3 --profile cnh-sso sync <local-folder>/v1/ s3://<bucket-name>/<repo-name>/v1/ --dryrun Generate MD5 checksums for uploaded files Run the following command from your local machine where the data files are stored: md5sum * > md5sum.txt Upload the md5sum.txt file to the same S3 folder. aws s3 cp md5sum.txt s3://<bucket-name>/<repo-name>/v1/ Update download_data.sh in the GitHub repository Pull the latest version of the repository locally: git pull origin main Edit download_data.sh to add the newly released version folder. For example, for version v1: RELEASE=${RELEASE:-v1} Test the updated download script locally to ensure correct download of files: bash download_data.sh Commit and push your changes: git add download_data.sh git commit -m \"Add v1 data release to download_data.sh\" git push origin <branch-name> Open a Pull Request with a description of the data release changes. Creating submodules The following is a good resource: https://git-scm.com/book/en/v2/Git-Tools-Submodules Below is documentation from Kelsey Keith at CHOP: ## Git Submodules Followed the instructions in the git book for setting up a submodule, <https://git-scm.com/book/en/v2/Git-Tools-Submodules> ### Toy Example of How to Set up a Submodule *2022-09-02* Setting up a repo as a submodule of another repo. I used two personal private toy repositories `practice` and `old_practice_repository` ```bash git clone https://github.com/kelseykeith/practice.git cd practice git submodule add https://github.com/kelseykeith/old_practice_repository git submodule init git submodule update Go to the other repository, make some changes, and then you can sync them into the other repository it's a submodule of git clone https://github.com/kelseykeith/old_practice_repository cd old_practice_repository echo 'submodule testing' > submodule_test.txt git add submodule_test.txt git commit -m 'using this repo and the practice repository repo to practice adding a git submodule' git push cd /path/to/practice/repository git submodule update --remote --merge Setting up OpenPedCan-analysis as a submodule of documentation so that I can reference OpenPedCan-analysis files cd documentation git checkout main git checkout -b table-update git submodule add https://github.com/PediatricOpenTargets/OpenPedCan-analysis.git # reset submodule to the last v10/v1.0.0 commit git reset --hard c0e5692e2949ad30b8e087ff0ec4e9385bde4b13","title":"Using GitHub"},{"location":"github-general/#using-github","text":"For general information on using GitHub, see here .","title":"Using GitHub"},{"location":"github-general/#setting-up-a-local-ssh-key-and-adding-to-github","text":"On your local computer (or EC2 instance for the first time), run the following command to generate your public SSH key. ssh-keygen <enter> <enter> <enter> cat ~/.ssh/id_rsa.pub Add this new SSH key to your profile in GitHub by following these instructions .","title":"Setting up a local SSH Key and adding to GitHub"},{"location":"github-general/#setting-up-a-github-repository","text":"Clone remote GitHub repo onto local computer using SSH: git clone git@github.com:rokitalab/OpenPedCan-Project-CNH.git Obtain latest version of project Docker image using Docker or Podman: docker pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest podman pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Run the Docker container For Local Development : docker run --name <CONTAINER_NAME> --platform linux/amd64 -d -e PASSWORD=pass -p 8787:8787 -v $PWD:/home/rstudio/OpenPedCan-Project-CNH pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Alternatively, the container can be initialized through the Docker dashboard on desktop. Mac and Linux users can also run Rstudio in the project docker container from a web browser. After executing the above docker run command, navigate to localhost:8787 in your web browser. The username for login is rstudio and the password will be whatever password is set in the docker run command above (default: pass ) For development using Amazon EC2 : docker run --platform linux/amd64 --name <CONTAINER_NAME> -d -e PASSWORD=pass -p 80:8787 -v $PWD:/home/rstudio/OpenPedCan-Project-CNH pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest To launch RStudio in a browser, enter the IP address in a web browser. The username for login is rstudio and the password is pass (default) or whatever was specified in the docker run command. (Optional) Once running, a bash shell can be opened into the container to run analyses. For example if <CONTAINER NAME> = openpedcan, run the following command from the root directory: docker exec -ti openpedcan bash From here, you can navigate to the module of interest as follows: cd /home/rstudio/OpenPedCan-Project-CNH/analyses/module-of-interest Download project data Data can typically be downloaded via a download-data.sh shell script in the project root directory: bash download-data.sh This will download all project data from the associated Amazon S3 bucket and create symlinks in data/ to the latest data release version. This command can be re-run to ensure that the most updated files are downloaded.","title":"Setting up a GitHub repository"},{"location":"github-general/#data-release-instructions","text":"For generating a new versioned data release, follow the steps below: Create a release-notes.md file This file should document the contents of the release, including: - Version name (e.g., v1) - Release date - Description of each data file - Any relevant changelog-style notes Use the format from the OpenPedCan project as a reference: OpenPedCan release-notes.md example Place a copy of release-notes.md in the GitHub repo under: doc/release-notes.md Create a versioned folder in S3 Create the new version folder in the designated S3 bucket: s3://<bucket-name>/<repo-name>/v1/ Add all finalized data files for this release into the versioned folder. Use --dryrun to test the copy from local to S3, and then remove it once you are sure the copy will go to the correct location. aws s3 --profile cnh-sso sync <local-folder>/v1/ s3://<bucket-name>/<repo-name>/v1/ --dryrun Generate MD5 checksums for uploaded files Run the following command from your local machine where the data files are stored: md5sum * > md5sum.txt Upload the md5sum.txt file to the same S3 folder. aws s3 cp md5sum.txt s3://<bucket-name>/<repo-name>/v1/ Update download_data.sh in the GitHub repository Pull the latest version of the repository locally: git pull origin main Edit download_data.sh to add the newly released version folder. For example, for version v1: RELEASE=${RELEASE:-v1} Test the updated download script locally to ensure correct download of files: bash download_data.sh Commit and push your changes: git add download_data.sh git commit -m \"Add v1 data release to download_data.sh\" git push origin <branch-name> Open a Pull Request with a description of the data release changes.","title":"Data release instructions"},{"location":"github-general/#creating-submodules","text":"The following is a good resource: https://git-scm.com/book/en/v2/Git-Tools-Submodules Below is documentation from Kelsey Keith at CHOP: ## Git Submodules Followed the instructions in the git book for setting up a submodule, <https://git-scm.com/book/en/v2/Git-Tools-Submodules> ### Toy Example of How to Set up a Submodule *2022-09-02* Setting up a repo as a submodule of another repo. I used two personal private toy repositories `practice` and `old_practice_repository` ```bash git clone https://github.com/kelseykeith/practice.git cd practice git submodule add https://github.com/kelseykeith/old_practice_repository git submodule init git submodule update Go to the other repository, make some changes, and then you can sync them into the other repository it's a submodule of git clone https://github.com/kelseykeith/old_practice_repository cd old_practice_repository echo 'submodule testing' > submodule_test.txt git add submodule_test.txt git commit -m 'using this repo and the practice repository repo to practice adding a git submodule' git push cd /path/to/practice/repository git submodule update --remote --merge Setting up OpenPedCan-analysis as a submodule of documentation so that I can reference OpenPedCan-analysis files cd documentation git checkout main git checkout -b table-update git submodule add https://github.com/PediatricOpenTargets/OpenPedCan-analysis.git # reset submodule to the last v10/v1.0.0 commit git reset --hard c0e5692e2949ad30b8e087ff0ec4e9385bde4b13","title":"Creating submodules"},{"location":"github-pr-guide/","text":"GitHub Code and Pull Request (PR) Guidelines We prefer the use of tidyverse functions over base R, when possible. The reason for this is so that all developers and reviewers are speaking the same language, making coding and reviewing easier across the team. Additional style guidelines We follow the tidyverse style guide for writing code. Avoid CamelCase unless already in a package function being used. Preparing Code for a Pull Request (PR) and Review - Some Principles Create quantifiable and transparent analysis goals Usually PRs will be linked to an issue (bug fix, new module development, module rerun with new data) or a scientific question we are seeking to answer. Clearly define this ahead of time by submitting an issue to track what you\u2019d like to do/have done or keep this in mind when creating a new branch for a future review. Use informative commit messages \u201cUpdate 01-new-analysis-01.R\u201d does not adequately explain what was done in that commit. It will be harder for you to go back and find where you changed something and harder for the reviewer to understand what was changed in that commit. Something like \u201cfix bug in survival cox ph function\u201d is more informative. Additional (bulleted) information can be added to the within the commit message if more detail is needed. See \u201cWhy good commit messages matter\u201d . Keep PRs \u2264 400 lines of code (LOC) In practice, a review of 200-400 LOC over 60 to 90 minutes should yield 70-90% defect discovery. Therefore, we aim to have PRs within this 200-400 LOC limit. Separate your scripts by purpose, and stack the PRs. Frequently, an analysis module has multiple scripts. Sometimes a module contains a new function. In these cases, you should separate your PRs by script, stacking them in the order in which they should be reviewed. For more on stacking PRs, see below. See our repository structure guidelines. Annotate your code. This should be done throughout the code, explaining each code chunk in a notebook and/or steps for an analysis. Reviewers will find fewer bugs/mistakes when authors read through and annotate their own code before submitting it for review. Add a module README. See OpenPedCan's Documenting Your Analysis . Add a module run script. Usually this is a shell script which runs all of the module scripts in order. Clearly document your goals within the PR template. Remember, a reviewer may not have the background of your project or previous analyses leading up to the PR in question, so it is very helpful to the reviewer when the PR is clearly described. Check out a new branch To make local changes to project repo, checkout a new branch: git switch -c <new-branch> Make desired changes to project repo in new branch (usually one script per PR) Add new analysis module Edit an existing module (add/edit scripts, etc.) Add new packages to project Dockerfile Add data files for new data release Change branch index to reflect any local changes git add <FILE> for any new or edited files or git add -A to add all changed files git rm <FILE> for any deleted files Commit local changes to branch, and includes a message describing what changes were made git commit -m \"message-regarding-changes\" Push commit(s) to remote repository git push origin <new-branch> This will push commits to remote branch new-branch , which will be created if it does not already exist. If authentication is required, the user needs to do the following: Go to GitHub page \u2192 personal account/Settings/Developer settings choose tokens (classic) \u2192 Generate new token Once in the terminal and asked for authentication, use the token as password. Update local git repo To download all changes from remote repo to local repo, use: git fetch To download changes AND merge them into current branch, use: git pull Submitting a Pull Request (PR) Any changes pushed to a project repo branch must be reviewed by team members prior to merging into another branch. This is accomplished by submitting a pull request (PR) that compares the new branch to the one you would like to merge into, and describes changes made. Navigate to remote project repo, and click \u201cPull Requests\u201d: Click \u201cNew pull request\u201d Select two branches to compare. The base branch should be the branch in which changes were made, and the compare branch should be the one into which you would like to merge. For example if you wanted to merge branch v12-tpm into dev : Click \u201cCreate pull request\u201d. This will navigate you to a page where you can provide a brief title and a description that reflects changes. At minimum, you should give details on: 1) changes made to repository and their motivation, 2) what kind of feedback you are seeking from reviewers, and 3) anything else that should be discussed regarding analyses and other changes. If a markdown template is in place (see example below), all questions should be answered prior to submitting PR. Once completed, click \u201cCreate pull request\u201d. The PR should now be visible if you navigate to open PRs. Add reviewers and labels to PR. There is a sidebar on the \u201cOpen a pull request\u201d page and PR page that allows you to choose reviewers and add labels to PR. To add reviewers, click \u201cReviewers\u201d and type in GitHub username of desired reviewer To add labels, click \u201cLabels\u201d and add desired label (e.g., \u201cReady for Review\u201d, \u201cBlocked\u201d, \u201cStacked\u201d). Stacking PRs To stack a PR, checkout the branch on which you would like to stack your next PR: git switch branch-1 Then, from within that branch, and not main/master/dev, create a new branch for the stacked PR: git switch branch-2 Once you create your PR with the second branch, then change the base branch to branch-1 . Now, you should see only the changes in branch-2 in your PR. These PRs either have to be merged in stacked order or they need to be merged backwards. If merging in order, then be sure to delete the branch post merge of each PR. This will automatically trigger changing of the base branch to main or master or dev in preparation for the next PR merge. If merging backwards, such as with data releases, they should all be approved first, and keep note that the number of changed files will get larger, making it harder to track.","title":"GitHub Pull Request Guidelines"},{"location":"github-pr-guide/#github-code-and-pull-request-pr-guidelines","text":"We prefer the use of tidyverse functions over base R, when possible. The reason for this is so that all developers and reviewers are speaking the same language, making coding and reviewing easier across the team. Additional style guidelines We follow the tidyverse style guide for writing code. Avoid CamelCase unless already in a package function being used.","title":"GitHub Code and Pull Request (PR) Guidelines"},{"location":"github-pr-guide/#preparing-code-for-a-pull-request-pr-and-review-some-principles","text":"Create quantifiable and transparent analysis goals Usually PRs will be linked to an issue (bug fix, new module development, module rerun with new data) or a scientific question we are seeking to answer. Clearly define this ahead of time by submitting an issue to track what you\u2019d like to do/have done or keep this in mind when creating a new branch for a future review. Use informative commit messages \u201cUpdate 01-new-analysis-01.R\u201d does not adequately explain what was done in that commit. It will be harder for you to go back and find where you changed something and harder for the reviewer to understand what was changed in that commit. Something like \u201cfix bug in survival cox ph function\u201d is more informative. Additional (bulleted) information can be added to the within the commit message if more detail is needed. See \u201cWhy good commit messages matter\u201d . Keep PRs \u2264 400 lines of code (LOC) In practice, a review of 200-400 LOC over 60 to 90 minutes should yield 70-90% defect discovery. Therefore, we aim to have PRs within this 200-400 LOC limit. Separate your scripts by purpose, and stack the PRs. Frequently, an analysis module has multiple scripts. Sometimes a module contains a new function. In these cases, you should separate your PRs by script, stacking them in the order in which they should be reviewed. For more on stacking PRs, see below. See our repository structure guidelines. Annotate your code. This should be done throughout the code, explaining each code chunk in a notebook and/or steps for an analysis. Reviewers will find fewer bugs/mistakes when authors read through and annotate their own code before submitting it for review. Add a module README. See OpenPedCan's Documenting Your Analysis . Add a module run script. Usually this is a shell script which runs all of the module scripts in order. Clearly document your goals within the PR template. Remember, a reviewer may not have the background of your project or previous analyses leading up to the PR in question, so it is very helpful to the reviewer when the PR is clearly described.","title":"Preparing Code for a Pull Request (PR) and Review - Some Principles"},{"location":"github-pr-guide/#check-out-a-new-branch","text":"To make local changes to project repo, checkout a new branch: git switch -c <new-branch> Make desired changes to project repo in new branch (usually one script per PR) Add new analysis module Edit an existing module (add/edit scripts, etc.) Add new packages to project Dockerfile Add data files for new data release Change branch index to reflect any local changes git add <FILE> for any new or edited files or git add -A to add all changed files git rm <FILE> for any deleted files Commit local changes to branch, and includes a message describing what changes were made git commit -m \"message-regarding-changes\" Push commit(s) to remote repository git push origin <new-branch> This will push commits to remote branch new-branch , which will be created if it does not already exist. If authentication is required, the user needs to do the following: Go to GitHub page \u2192 personal account/Settings/Developer settings choose tokens (classic) \u2192 Generate new token Once in the terminal and asked for authentication, use the token as password.","title":"Check out a new branch"},{"location":"github-pr-guide/#update-local-git-repo","text":"To download all changes from remote repo to local repo, use: git fetch To download changes AND merge them into current branch, use: git pull","title":"Update local git repo"},{"location":"github-pr-guide/#submitting-a-pull-request-pr","text":"Any changes pushed to a project repo branch must be reviewed by team members prior to merging into another branch. This is accomplished by submitting a pull request (PR) that compares the new branch to the one you would like to merge into, and describes changes made. Navigate to remote project repo, and click \u201cPull Requests\u201d: Click \u201cNew pull request\u201d Select two branches to compare. The base branch should be the branch in which changes were made, and the compare branch should be the one into which you would like to merge. For example if you wanted to merge branch v12-tpm into dev : Click \u201cCreate pull request\u201d. This will navigate you to a page where you can provide a brief title and a description that reflects changes. At minimum, you should give details on: 1) changes made to repository and their motivation, 2) what kind of feedback you are seeking from reviewers, and 3) anything else that should be discussed regarding analyses and other changes. If a markdown template is in place (see example below), all questions should be answered prior to submitting PR. Once completed, click \u201cCreate pull request\u201d. The PR should now be visible if you navigate to open PRs. Add reviewers and labels to PR. There is a sidebar on the \u201cOpen a pull request\u201d page and PR page that allows you to choose reviewers and add labels to PR. To add reviewers, click \u201cReviewers\u201d and type in GitHub username of desired reviewer To add labels, click \u201cLabels\u201d and add desired label (e.g., \u201cReady for Review\u201d, \u201cBlocked\u201d, \u201cStacked\u201d).","title":"Submitting a Pull Request (PR)"},{"location":"github-pr-guide/#stacking-prs","text":"To stack a PR, checkout the branch on which you would like to stack your next PR: git switch branch-1 Then, from within that branch, and not main/master/dev, create a new branch for the stacked PR: git switch branch-2 Once you create your PR with the second branch, then change the base branch to branch-1 . Now, you should see only the changes in branch-2 in your PR. These PRs either have to be merged in stacked order or they need to be merged backwards. If merging in order, then be sure to delete the branch post merge of each PR. This will automatically trigger changing of the base branch to main or master or dev in preparation for the next PR merge. If merging backwards, such as with data releases, they should all be approved first, and keep note that the number of changed files will get larger, making it harder to track.","title":"Stacking PRs"},{"location":"github-repo-guide/","text":"GitHub Repository Guidelines In order to facilitate rapid collaboration across many different projects, we have adapted guidelines for our analysis projects, including the repository structure, docker image principles, a pull request model, and code review. Repository guidelines (see OpenPedCan documentation ) Repository Docker Image We set up all of our repositories to use project-specific Docker images containing an instance of RStudio and tidyverse (R > 4.4) from the Rocker project . rocker/tidyverse has already installed many R packages and their dependencies\u2019 apt packages. e.g. the tidyverse package , the devtools package , the rmarkdown package , some R Database Interface packages, the data.table package , the fst package , and the Apache Arrow R package . More on docker here . Repository folder structure Users performing analyses should always refer to the symlinks in the data/ directory and not files within the release folder, as an updated release may be produced before a publication is prepared. The repository folder structure is designed to separate each analysis into its own set of notebooks that are independent of other analyses. Within the analyses directory, create a folder for your analysis. Choose a name that is unique from other analyses and somewhat detailed. For example, instead of gene-expression , choose gene-expression-clustering if you are clustering samples by their gene expression values. You should assume that any data files are in the ../../data directory and that their file names match what the download-data.sh script produces. These files should be read in at their relative path, so that we can re-run analyses if the underlying data change. Files that are primarily graphic should be placed in a plots subdirectory and should adhere to a color palette guide for your project. Files that are primarily tabular results files should be placed in a results subdirectory. Intermediate files that are useful within the processing steps but that do not represent final results should be placed in ../../scratch/ . It is safe to assume that files placed in ../../scratch will be available to all analyses within the same folder. It is not safe to assume that files placed in ../../scratch will be available from analyses in a different folder. An example highlighting a new-analysis directory is shown below. The directory is placed alongside existing analyses within the analyses directory. In this case, the author of the analysis has run their workflows in R Markdown notebooks. This is denoted with the .Rmd suffix. However, the author could have used Jupyter notebooks, R scripts, or another scriptable solution. The author has created a new function or set of functions and placed those into new-function.R which lives in the util folder of the new-analysis folder. The author has produced their output figures as .pdf files. We have a preference for vector graphics as PDF files, though other forms of vector graphics are also appropriate. The results folder contains a tabular summary as a comma separated values file. We expect that the file suffix ( .csv , .tsv ) accurately denotes the format of the added files. The author has also included a README.md ( see Documenting Your Analysis ). OpenPedCan-analysis \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u2502 \u251c\u2500\u2500 existing-analysis-1 \u2502 \u2514\u2500\u2500 new-analysis \u2502 \u251c\u2500\u2500 01-preprocess-data.Rmd \u2502 \u251c\u2500\u2500 02-run-analyses.Rmd \u2502 \u251c\u2500\u2500 03-make-figures.Rmd \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 plots \u2502 \u2502 \u251c\u2500\u2500 figure1.pdf \u2502 \u2502 \u2514\u2500\u2500 figure2.pdf \u2502 \u251c\u2500\u2500 util \u2502 \u2502 \u2514\u2500\u2500 new-function.R \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u2514\u2500\u2500 tabular_summary.csv \u2502 \u2514\u2500\u2500 run-new-analysis.sh \u251c\u2500\u2500 data \u251c\u2500\u2500 download-data.sh \u251c\u2500\u2500 figures \u2514\u2500\u2500 scratch","title":"GitHub Repository Guidelines"},{"location":"github-repo-guide/#github-repository-guidelines","text":"In order to facilitate rapid collaboration across many different projects, we have adapted guidelines for our analysis projects, including the repository structure, docker image principles, a pull request model, and code review.","title":"GitHub Repository Guidelines"},{"location":"github-repo-guide/#repository-guidelines-see-openpedcan-documentation","text":"Repository Docker Image We set up all of our repositories to use project-specific Docker images containing an instance of RStudio and tidyverse (R > 4.4) from the Rocker project . rocker/tidyverse has already installed many R packages and their dependencies\u2019 apt packages. e.g. the tidyverse package , the devtools package , the rmarkdown package , some R Database Interface packages, the data.table package , the fst package , and the Apache Arrow R package . More on docker here . Repository folder structure Users performing analyses should always refer to the symlinks in the data/ directory and not files within the release folder, as an updated release may be produced before a publication is prepared. The repository folder structure is designed to separate each analysis into its own set of notebooks that are independent of other analyses. Within the analyses directory, create a folder for your analysis. Choose a name that is unique from other analyses and somewhat detailed. For example, instead of gene-expression , choose gene-expression-clustering if you are clustering samples by their gene expression values. You should assume that any data files are in the ../../data directory and that their file names match what the download-data.sh script produces. These files should be read in at their relative path, so that we can re-run analyses if the underlying data change. Files that are primarily graphic should be placed in a plots subdirectory and should adhere to a color palette guide for your project. Files that are primarily tabular results files should be placed in a results subdirectory. Intermediate files that are useful within the processing steps but that do not represent final results should be placed in ../../scratch/ . It is safe to assume that files placed in ../../scratch will be available to all analyses within the same folder. It is not safe to assume that files placed in ../../scratch will be available from analyses in a different folder. An example highlighting a new-analysis directory is shown below. The directory is placed alongside existing analyses within the analyses directory. In this case, the author of the analysis has run their workflows in R Markdown notebooks. This is denoted with the .Rmd suffix. However, the author could have used Jupyter notebooks, R scripts, or another scriptable solution. The author has created a new function or set of functions and placed those into new-function.R which lives in the util folder of the new-analysis folder. The author has produced their output figures as .pdf files. We have a preference for vector graphics as PDF files, though other forms of vector graphics are also appropriate. The results folder contains a tabular summary as a comma separated values file. We expect that the file suffix ( .csv , .tsv ) accurately denotes the format of the added files. The author has also included a README.md ( see Documenting Your Analysis ). OpenPedCan-analysis \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u2502 \u251c\u2500\u2500 existing-analysis-1 \u2502 \u2514\u2500\u2500 new-analysis \u2502 \u251c\u2500\u2500 01-preprocess-data.Rmd \u2502 \u251c\u2500\u2500 02-run-analyses.Rmd \u2502 \u251c\u2500\u2500 03-make-figures.Rmd \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 plots \u2502 \u2502 \u251c\u2500\u2500 figure1.pdf \u2502 \u2502 \u2514\u2500\u2500 figure2.pdf \u2502 \u251c\u2500\u2500 util \u2502 \u2502 \u2514\u2500\u2500 new-function.R \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u2514\u2500\u2500 tabular_summary.csv \u2502 \u2514\u2500\u2500 run-new-analysis.sh \u251c\u2500\u2500 data \u251c\u2500\u2500 download-data.sh \u251c\u2500\u2500 figures \u2514\u2500\u2500 scratch","title":"Repository guidelines (see OpenPedCan documentation)"},{"location":"github-tokens/","text":"Github Tokens This page lists the various tokens that are used within the Rokita Lab and the BTI Bioinformatics Core organizations through secrets. If you are going to be managing tokens, you may need to request additional permission in the organizations and repos in which you plan to use the tokens. The tokens listed here are stored in the repos as secrets and used by different Actions. You don't need to include personal tokens you use locally here. Generating Token Documentation GitHub token documentation can be found here . Token Table Token Name Organization Expiration Date Repos Name in Repo Action Permissions Notes github_add_item_token childrens-bti July 1, 2026 All ADD_TO_PROJECT_PAT add-issues-to-project Organization permissions: Read and Write access to organization projects. Repository permissions: Read access to metadata; Read and Write access to issues and pull requests. This is an organization level token stored within childrens-bti and rokita-lab cavatica_api_wrappers_my_repo_pat childrens-bti July 1, 2026 cavatica_api_wrappers MY_REPO_PAT make-cavatica-project Repository permissions: Read access to code and metadata. file-manifest-shiny-my_repo_pat childrens-bti April 13 2026 file-manifest-shiny MY_REPO_PAT deploy-shinyapp and test-shinyapp Repository permissions: Read access to actions, code, metadata, and secrets CAVATICA_TOKEN rokitalab NA OpenPedCan-Project-CNH CAVATICA_TOKEN build and build_and_push Repository permissions: log into CAVATICA and push docker image. Value is created on Cavatica not in github. CAVATICA_USERNAME rokitalab NA OpenPedCan-Project-CNH CAVATICA_USERNAME build and build_and_push Repository permissions: log into CAVATICA and push docker image. Value is created on Cavatica not in github.","title":"Github Tokens"},{"location":"github-tokens/#github-tokens","text":"This page lists the various tokens that are used within the Rokita Lab and the BTI Bioinformatics Core organizations through secrets. If you are going to be managing tokens, you may need to request additional permission in the organizations and repos in which you plan to use the tokens. The tokens listed here are stored in the repos as secrets and used by different Actions. You don't need to include personal tokens you use locally here.","title":"Github Tokens"},{"location":"github-tokens/#generating-token-documentation","text":"GitHub token documentation can be found here .","title":"Generating Token Documentation"},{"location":"github-tokens/#token-table","text":"Token Name Organization Expiration Date Repos Name in Repo Action Permissions Notes github_add_item_token childrens-bti July 1, 2026 All ADD_TO_PROJECT_PAT add-issues-to-project Organization permissions: Read and Write access to organization projects. Repository permissions: Read access to metadata; Read and Write access to issues and pull requests. This is an organization level token stored within childrens-bti and rokita-lab cavatica_api_wrappers_my_repo_pat childrens-bti July 1, 2026 cavatica_api_wrappers MY_REPO_PAT make-cavatica-project Repository permissions: Read access to code and metadata. file-manifest-shiny-my_repo_pat childrens-bti April 13 2026 file-manifest-shiny MY_REPO_PAT deploy-shinyapp and test-shinyapp Repository permissions: Read access to actions, code, metadata, and secrets CAVATICA_TOKEN rokitalab NA OpenPedCan-Project-CNH CAVATICA_TOKEN build and build_and_push Repository permissions: log into CAVATICA and push docker image. Value is created on Cavatica not in github. CAVATICA_USERNAME rokitalab NA OpenPedCan-Project-CNH CAVATICA_USERNAME build and build_and_push Repository permissions: log into CAVATICA and push docker image. Value is created on Cavatica not in github.","title":"Token Table"},{"location":"literature/","text":"Background and Literature Here, we have compiled some important literature related to pediatric CNS brain tumors and computational workflows our group utilizes. Please review. Consortia The Children's Brain Tumor Network (CBTN) The CBTN is a multi-institutional international clinical research consortium which was created to advance therapeutic development for children with central nervous system tumors through the collection and rapid distribution of biospecimens and data. In 2018, the CBTN released WGS and RNA-Seq for ~1,000 tumors for researchers without embargo. Read the CBTN manuscript here . The CBTN also has many cell line and organoid models available to researchers. CBTN model descriptions, genomics, and characteristics can be viewed here . The Pediatric Neuro-Oncology Consortium (PNOC) The PNOC is an international consortium dedicated to advancing clinical paradigms and therapy for children and young adults with brain tumors. Clinical Proteomic Tumor Analysis Consortium (CPTAC) The CPTAC is an NCI effort \"to accelerate the understanding of the molecular basis of cancer through the application of large-scale proteome and genome analysis, or proteogenomics\". For more information, see here . Open Source Platforms and Tools Gabriella Miller Kids First Data Resource Center (GMKF DRC) We utilize the DNA and RNA sequencing workflows benchmarked and created by the Kids First DRC , which operates at the Children's Hospital of Philadelphia. The code for the workflows can be found in the Kids First GitHub repository with corresponding CWL applications are publicly available within CAVATICA. OpenPBTA and OpenPedCan The Open Pediatric Brain Tumor Atlas (OpenPBTA) was a global open-source analysis effort to analyze those first 1,000 PBTA tumors released by the CBTN and the PNOC003 DIPG clinical trial. The OpenPBTA was led and maintained by researchers at CHOP and the Childhood Cancer Data Lab (CCDL) at Alex's Lemonade Stand Foundation. Read the OpenPBTA manuscript here and visit the archived GitHub repository here . The OpenPBTA was later expanded to include additional pediatric cancer genomic data as well as gene expression data from TCGA and GTEx cohorts through the Open Pediatric Cancer (OpenPedcan) Project. Read the OpenPedCan manuscript here . OpenScPCA ALSF supported data generation, deposition, and harmonization of multiple pediatric cancer single cell studies, which are now housed in the Single-cell Pediatric Cancer Atlas Portal . The CCDL created the Open Single-cell Pediatric Cell Atlas (OpenScPCA) analysis project to analyze these data. CPTAC Data Portals CPTAC raw data files can be explored on the Proteomic Data Commons (PDC) . A manifest can be generated from the PDC and files may be imported to CAVATICA for easy access. In support of the collaborative publication between CBTN and CPTAC, \"Integrated Proteogenomic Characterization across Major Histological Types of Pediatric Brain Cancer\" , a web portal was created for easy multi-omic visualization and data download. The portal can be found here . CNS Tumor Literature Pediatric CNS Tumor Classification The World Health Organization (WHO) introduced \"integrated diagnoses\" for CNS tumors in 2016 , followed by a significant update in 2021 . Today, there are still new entities and predispositions being discovered. Most of these tumors can be classified by running methylation arrays through either the DKFZ classifier or the NCI classifier . Cancer-specific marker publications This section is under construction.","title":"Literature"},{"location":"literature/#background-and-literature","text":"Here, we have compiled some important literature related to pediatric CNS brain tumors and computational workflows our group utilizes. Please review.","title":"Background and Literature"},{"location":"literature/#consortia","text":"","title":"Consortia"},{"location":"literature/#the-childrens-brain-tumor-network-cbtn","text":"The CBTN is a multi-institutional international clinical research consortium which was created to advance therapeutic development for children with central nervous system tumors through the collection and rapid distribution of biospecimens and data. In 2018, the CBTN released WGS and RNA-Seq for ~1,000 tumors for researchers without embargo. Read the CBTN manuscript here . The CBTN also has many cell line and organoid models available to researchers. CBTN model descriptions, genomics, and characteristics can be viewed here .","title":"The Children's Brain Tumor Network (CBTN)"},{"location":"literature/#the-pediatric-neuro-oncology-consortium-pnoc","text":"The PNOC is an international consortium dedicated to advancing clinical paradigms and therapy for children and young adults with brain tumors.","title":"The Pediatric Neuro-Oncology Consortium (PNOC)"},{"location":"literature/#clinical-proteomic-tumor-analysis-consortium-cptac","text":"The CPTAC is an NCI effort \"to accelerate the understanding of the molecular basis of cancer through the application of large-scale proteome and genome analysis, or proteogenomics\". For more information, see here .","title":"Clinical Proteomic Tumor Analysis Consortium (CPTAC)"},{"location":"literature/#open-source-platforms-and-tools","text":"","title":"Open Source Platforms and Tools"},{"location":"literature/#gabriella-miller-kids-first-data-resource-center-gmkf-drc","text":"We utilize the DNA and RNA sequencing workflows benchmarked and created by the Kids First DRC , which operates at the Children's Hospital of Philadelphia. The code for the workflows can be found in the Kids First GitHub repository with corresponding CWL applications are publicly available within CAVATICA.","title":"Gabriella Miller Kids First Data Resource Center (GMKF DRC)"},{"location":"literature/#openpbta-and-openpedcan","text":"The Open Pediatric Brain Tumor Atlas (OpenPBTA) was a global open-source analysis effort to analyze those first 1,000 PBTA tumors released by the CBTN and the PNOC003 DIPG clinical trial. The OpenPBTA was led and maintained by researchers at CHOP and the Childhood Cancer Data Lab (CCDL) at Alex's Lemonade Stand Foundation. Read the OpenPBTA manuscript here and visit the archived GitHub repository here . The OpenPBTA was later expanded to include additional pediatric cancer genomic data as well as gene expression data from TCGA and GTEx cohorts through the Open Pediatric Cancer (OpenPedcan) Project. Read the OpenPedCan manuscript here .","title":"OpenPBTA and OpenPedCan"},{"location":"literature/#openscpca","text":"ALSF supported data generation, deposition, and harmonization of multiple pediatric cancer single cell studies, which are now housed in the Single-cell Pediatric Cancer Atlas Portal . The CCDL created the Open Single-cell Pediatric Cell Atlas (OpenScPCA) analysis project to analyze these data.","title":"OpenScPCA"},{"location":"literature/#cptac-data-portals","text":"CPTAC raw data files can be explored on the Proteomic Data Commons (PDC) . A manifest can be generated from the PDC and files may be imported to CAVATICA for easy access. In support of the collaborative publication between CBTN and CPTAC, \"Integrated Proteogenomic Characterization across Major Histological Types of Pediatric Brain Cancer\" , a web portal was created for easy multi-omic visualization and data download. The portal can be found here .","title":"CPTAC Data Portals"},{"location":"literature/#cns-tumor-literature","text":"","title":"CNS Tumor Literature"},{"location":"literature/#pediatric-cns-tumor-classification","text":"The World Health Organization (WHO) introduced \"integrated diagnoses\" for CNS tumors in 2016 , followed by a significant update in 2021 . Today, there are still new entities and predispositions being discovered. Most of these tumors can be classified by running methylation arrays through either the DKFZ classifier or the NCI classifier .","title":"Pediatric CNS Tumor Classification"},{"location":"literature/#cancer-specific-marker-publications","text":"This section is under construction.","title":"Cancer-specific marker publications"},{"location":"project-mgmt/","text":"BTI Project Management Scrum In the Rokita Lab and the BTI Bioinformatics Core, we are implementing Scrum as a project management framework. To get familiar with Scrum please refer to the Scrum Guide or Scrum Essentials in Under 10 Minutes . Sprint Planning For an overview on sprints, see sprints . Sprint Planning along with Review and Retrospective happen bi-weekly on Tuesdays. Review and Retrospective begin at 11 am and Planning is at 1pm. For access to the meetings, please ask either Dr. Rokita or Alex Sickler. GitHub Project Project management is done through GitHub. Issues can be created by anyone with access to a repository. They provide a way to track, plan, and discuss work. Issues can be created in each repo or in the Internal Ticket Tracker . The Internal Ticket Tracker serves as a repository just for issues and contains issue templates for a number of common tasks that we work on and can be used for general tasks. Issues may be created in a repository for items related to that repository such as feature requests or bug reports. If you're unsure whether an issue should be created in a repo or the Internal Ticket Tracker, create the issue in the Internal Ticket Tracker. Issues are organized with information on the task being tracked, labels, type, assignees, and project. Within a project, issues can be given priority, size, iteration, and status. The BTI Bioinformatics Projects serves as the main board and is used for tracking issue and PR statuses and team members workfloads. Issues and PRs can be added to the project. Periodically, all issues and PRs in the childrens-bti and the rokitalab organizations are added to this project. Issues and PRs may be assigned to multiple projects.","title":"BTI Project Management"},{"location":"project-mgmt/#bti-project-management","text":"","title":"BTI Project Management"},{"location":"project-mgmt/#scrum","text":"In the Rokita Lab and the BTI Bioinformatics Core, we are implementing Scrum as a project management framework. To get familiar with Scrum please refer to the Scrum Guide or Scrum Essentials in Under 10 Minutes .","title":"Scrum"},{"location":"project-mgmt/#sprint-planning","text":"For an overview on sprints, see sprints . Sprint Planning along with Review and Retrospective happen bi-weekly on Tuesdays. Review and Retrospective begin at 11 am and Planning is at 1pm. For access to the meetings, please ask either Dr. Rokita or Alex Sickler.","title":"Sprint Planning"},{"location":"project-mgmt/#github-project","text":"Project management is done through GitHub. Issues can be created by anyone with access to a repository. They provide a way to track, plan, and discuss work. Issues can be created in each repo or in the Internal Ticket Tracker . The Internal Ticket Tracker serves as a repository just for issues and contains issue templates for a number of common tasks that we work on and can be used for general tasks. Issues may be created in a repository for items related to that repository such as feature requests or bug reports. If you're unsure whether an issue should be created in a repo or the Internal Ticket Tracker, create the issue in the Internal Ticket Tracker. Issues are organized with information on the task being tracked, labels, type, assignees, and project. Within a project, issues can be given priority, size, iteration, and status. The BTI Bioinformatics Projects serves as the main board and is used for tracking issue and PR statuses and team members workfloads. Issues and PRs can be added to the project. Periodically, all issues and PRs in the childrens-bti and the rokitalab organizations are added to this project. Issues and PRs may be assigned to multiple projects.","title":"GitHub Project"},{"location":"sop_cwl_cavatica/","text":"SOP: Quality Assurance & Best Practices for Running CWL Workflows on Cavatica Version: 1.0 Date: 2026-01-15 Team: BTI-BFX-Engineering Table of Contents Purpose Scope Guiding Principles Pre-Run Task Preparation (Cavatica-Specific) File Inputs CWL Workflow Validation Versioning Requirements CWL Runtime Settings Workflow Design Standards Required Validation Steps Output Contract Logging Requirements Task Planning & Execution on Cavatica Small-Batch Validation Peer Review Criteria Before Full Launch Preventing Reruns Preventing Task Deletion Exporting Data Safely Documentation Requirements Continuous Improvement Roles & Responsibilities Appendices 1. Purpose This SOP defines standards and procedures for designing, validating, launching, and exporting data from CWL workflows run on Cavatica , focusing on reducing task deletions, reruns, and increasing workflow reliability. 2. Scope This SOP applies to all CWL workflows executed on Cavatica. 3. Guiding Principles Reproducibility Validation before execution Predictable outputs Immutability Traceability 4. Pre-Run Task Preparation (Cavatica-Specific) 4.1 File Inputs Validate file types and metadata Confirm file IDs Verify references 4.2 CWL Workflow Validation (Before Depolying to Cavatica) Use cwltool --validate Validate input schema 4.3 Versioning Requirements Document CWL version, Docker digest, reference bundle 4.4 CWL Runtime Settings Set resource requirements Avoid hard-coded paths 5. Workflow Design Standards 5.1 Required Validation Steps Input metadata validation Reference integrity checks File existence checks 5.2 Output Contract (Cavatica) Define final outputs Checksums Naming conventions 5.3 Logging Requirements Structured logs Summary log Docker stdout/stderr 5.4 Workflow I/O Documentation (File Types + Globs + Paths) Document expected input file extensions (e.g., .fastq.gz , .bam{,.bai} , .vcf.gz{,.tbi} , .json/.tsv ) Document expected output file extensions and where they land (e.g., outputs/** , logs/** , qc/** , checksums/** ) Include canonical glob patterns for discovery/validation (e.g., inputs/**/*.{fastq,fq}.gz , outputs/**/*.vcf.gz{,.tbi} ) List potential/allowed project paths (Cavatica project folders, mounted reference locations) and prohibit hard-coded absolute paths 6. Task Planning & Execution on Cavatica 6.1 Small-Batch Validation Run 1\u20133 samples end-to-end before full launch. 6.2 Peer Review Another engineer reviews inputs, versions, references. 6.3 Criteria Before Full Launch All validations passed, parameters confirmed. 7. Preventing Reruns Use version-locked references, docker digests, validation scripts. 8. Preventing Task Deletion (Cavatica Best Practices) Use dev projects for testing Enforce naming conventions Avoid overwriting outputs 9. Exporting Data Safely from Cavatica Pre-Export Validate outputs, checksums Post-Export Spot QC, document export details 10. Documentation Requirements README Input schema Output contract Changelog 11. Continuous Improvement Quarterly reviews, post-mortems. 12. Roles & Responsibilities Role Responsibility Engineering Workflow development Data Ops QC & exports Leads Approvals All Users SOP compliance 13. Appendices A. Sample Task Description Template Workflow: WGS Alignment v2.4.0 Commit: f1c2e7a Docker: quay.io/childrens-bti/wgs:v2.4.0@sha256:... Reference: GRCh38_refbundle_v1 Inputs validated: Yes Export path: s3://bti-data/harmonization/wgs/v2.4.0/ QC reviewer: name Run date: YYYY-MM-DD B. Metadata Schema Template (To be filled per workflow) C. Output Contract Example (To be added per workflow) End of Document","title":"CAVATICA workflow SOP"},{"location":"sop_cwl_cavatica/#sop-quality-assurance-best-practices-for-running-cwl-workflows-on-cavatica","text":"Version: 1.0 Date: 2026-01-15 Team: BTI-BFX-Engineering","title":"SOP: Quality Assurance &amp; Best Practices for Running CWL Workflows on Cavatica"},{"location":"sop_cwl_cavatica/#table-of-contents","text":"Purpose Scope Guiding Principles Pre-Run Task Preparation (Cavatica-Specific) File Inputs CWL Workflow Validation Versioning Requirements CWL Runtime Settings Workflow Design Standards Required Validation Steps Output Contract Logging Requirements Task Planning & Execution on Cavatica Small-Batch Validation Peer Review Criteria Before Full Launch Preventing Reruns Preventing Task Deletion Exporting Data Safely Documentation Requirements Continuous Improvement Roles & Responsibilities Appendices","title":"Table of Contents"},{"location":"sop_cwl_cavatica/#1-purpose","text":"This SOP defines standards and procedures for designing, validating, launching, and exporting data from CWL workflows run on Cavatica , focusing on reducing task deletions, reruns, and increasing workflow reliability.","title":"1. Purpose"},{"location":"sop_cwl_cavatica/#2-scope","text":"This SOP applies to all CWL workflows executed on Cavatica.","title":"2. Scope"},{"location":"sop_cwl_cavatica/#3-guiding-principles","text":"Reproducibility Validation before execution Predictable outputs Immutability Traceability","title":"3. Guiding Principles"},{"location":"sop_cwl_cavatica/#4-pre-run-task-preparation-cavatica-specific","text":"","title":"4. Pre-Run Task Preparation (Cavatica-Specific)"},{"location":"sop_cwl_cavatica/#41-file-inputs","text":"Validate file types and metadata Confirm file IDs Verify references","title":"4.1 File Inputs"},{"location":"sop_cwl_cavatica/#42-cwl-workflow-validation-before-depolying-to-cavatica","text":"Use cwltool --validate Validate input schema","title":"4.2 CWL Workflow Validation (Before Depolying to Cavatica)"},{"location":"sop_cwl_cavatica/#43-versioning-requirements","text":"Document CWL version, Docker digest, reference bundle","title":"4.3 Versioning Requirements"},{"location":"sop_cwl_cavatica/#44-cwl-runtime-settings","text":"Set resource requirements Avoid hard-coded paths","title":"4.4 CWL Runtime Settings"},{"location":"sop_cwl_cavatica/#5-workflow-design-standards","text":"","title":"5. Workflow Design Standards"},{"location":"sop_cwl_cavatica/#51-required-validation-steps","text":"Input metadata validation Reference integrity checks File existence checks","title":"5.1 Required Validation Steps"},{"location":"sop_cwl_cavatica/#52-output-contract-cavatica","text":"Define final outputs Checksums Naming conventions","title":"5.2 Output Contract (Cavatica)"},{"location":"sop_cwl_cavatica/#53-logging-requirements","text":"Structured logs Summary log Docker stdout/stderr","title":"5.3 Logging Requirements"},{"location":"sop_cwl_cavatica/#54-workflow-io-documentation-file-types-globs-paths","text":"Document expected input file extensions (e.g., .fastq.gz , .bam{,.bai} , .vcf.gz{,.tbi} , .json/.tsv ) Document expected output file extensions and where they land (e.g., outputs/** , logs/** , qc/** , checksums/** ) Include canonical glob patterns for discovery/validation (e.g., inputs/**/*.{fastq,fq}.gz , outputs/**/*.vcf.gz{,.tbi} ) List potential/allowed project paths (Cavatica project folders, mounted reference locations) and prohibit hard-coded absolute paths","title":"5.4 Workflow I/O Documentation (File Types + Globs + Paths)"},{"location":"sop_cwl_cavatica/#6-task-planning-execution-on-cavatica","text":"","title":"6. Task Planning &amp; Execution on Cavatica"},{"location":"sop_cwl_cavatica/#61-small-batch-validation","text":"Run 1\u20133 samples end-to-end before full launch.","title":"6.1 Small-Batch Validation"},{"location":"sop_cwl_cavatica/#62-peer-review","text":"Another engineer reviews inputs, versions, references.","title":"6.2 Peer Review"},{"location":"sop_cwl_cavatica/#63-criteria-before-full-launch","text":"All validations passed, parameters confirmed.","title":"6.3 Criteria Before Full Launch"},{"location":"sop_cwl_cavatica/#7-preventing-reruns","text":"Use version-locked references, docker digests, validation scripts.","title":"7. Preventing Reruns"},{"location":"sop_cwl_cavatica/#8-preventing-task-deletion-cavatica-best-practices","text":"Use dev projects for testing Enforce naming conventions Avoid overwriting outputs","title":"8. Preventing Task Deletion (Cavatica Best Practices)"},{"location":"sop_cwl_cavatica/#9-exporting-data-safely-from-cavatica","text":"","title":"9. Exporting Data Safely from Cavatica"},{"location":"sop_cwl_cavatica/#pre-export","text":"Validate outputs, checksums","title":"Pre-Export"},{"location":"sop_cwl_cavatica/#post-export","text":"Spot QC, document export details","title":"Post-Export"},{"location":"sop_cwl_cavatica/#10-documentation-requirements","text":"README Input schema Output contract Changelog","title":"10. Documentation Requirements"},{"location":"sop_cwl_cavatica/#11-continuous-improvement","text":"Quarterly reviews, post-mortems.","title":"11. Continuous Improvement"},{"location":"sop_cwl_cavatica/#12-roles-responsibilities","text":"Role Responsibility Engineering Workflow development Data Ops QC & exports Leads Approvals All Users SOP compliance","title":"12. Roles &amp; Responsibilities"},{"location":"sop_cwl_cavatica/#13-appendices","text":"","title":"13. Appendices"},{"location":"sop_cwl_cavatica/#a-sample-task-description-template","text":"Workflow: WGS Alignment v2.4.0 Commit: f1c2e7a Docker: quay.io/childrens-bti/wgs:v2.4.0@sha256:... Reference: GRCh38_refbundle_v1 Inputs validated: Yes Export path: s3://bti-data/harmonization/wgs/v2.4.0/ QC reviewer: name Run date: YYYY-MM-DD","title":"A. Sample Task Description Template"},{"location":"sop_cwl_cavatica/#b-metadata-schema-template","text":"(To be filled per workflow)","title":"B. Metadata Schema Template"},{"location":"sop_cwl_cavatica/#c-output-contract-example","text":"(To be added per workflow) End of Document","title":"C. Output Contract Example"},{"location":"sprints/","text":"Sprints Sprints are arguably the most important part of Scrum. In the Rokita Lab, sprints are two week periods starting one after the other every other Tuesday. Sprints allow a set time for work to be planned, organized, and acomplished. Sprints enable predictability within an organiazation and provide a digestible amount of expected progress. It's helpful to think of sprints as small projects. See the Scrum guide for more detail. On GitHub, sprints are called \"iteration\".","title":"Sprint Planning"},{"location":"sprints/#sprints","text":"Sprints are arguably the most important part of Scrum. In the Rokita Lab, sprints are two week periods starting one after the other every other Tuesday. Sprints allow a set time for work to be planned, organized, and acomplished. Sprints enable predictability within an organiazation and provide a digestible amount of expected progress. It's helpful to think of sprints as small projects. See the Scrum guide for more detail. On GitHub, sprints are called \"iteration\".","title":"Sprints"},{"location":"vscodium/","text":"Using VSCodium for Code Reviews There are multiple free text editors available which can be useful for code review, but I highlight the open-source editor, VSCodium here because of ease of use. Connecting VSCodium to your EC2 Instance In order to connect VSCodium to your EC2 instance, you will need to configure your local SSH profile. To do so, you need to update your config file located at ~/.ssh/config to include the private SSH key on your instance. From your instance, cat ~/.ssh/id_ed25519 and copy this into a new text file on your computer, for instance ~/.ssh/ec2_key . Create a config file in ~/.ssh/config if none exists, or simply add to it the information below: Host name_here HostName ip_address User ubuntu IdentityFile ~/.ssh/ec2_key Click on the tetris box-looking icon, search Open Remote - SSH , and install the plug-in. Within VSCodium, Click Connect to , then Connect to Host and Type in ubuntu@ip_address . You should be in! Code Review with VSCodium You can navigate to the repository of interest within VSCodium and any differences in your branch compared to the remote branch will now show up with different color codes as below. This is very handy when trying to get an overall view of differences in images and/or files with multiple differences. Below, on the left, multiple files have changes (M = modified) and on the right, there are differences between the local and remote file (red, deleted lines).","title":"Using VSCodium for Code Reviews"},{"location":"vscodium/#using-vscodium-for-code-reviews","text":"There are multiple free text editors available which can be useful for code review, but I highlight the open-source editor, VSCodium here because of ease of use.","title":"Using VSCodium for Code Reviews"},{"location":"vscodium/#connecting-vscodium-to-your-ec2-instance","text":"In order to connect VSCodium to your EC2 instance, you will need to configure your local SSH profile. To do so, you need to update your config file located at ~/.ssh/config to include the private SSH key on your instance. From your instance, cat ~/.ssh/id_ed25519 and copy this into a new text file on your computer, for instance ~/.ssh/ec2_key . Create a config file in ~/.ssh/config if none exists, or simply add to it the information below: Host name_here HostName ip_address User ubuntu IdentityFile ~/.ssh/ec2_key Click on the tetris box-looking icon, search Open Remote - SSH , and install the plug-in. Within VSCodium, Click Connect to , then Connect to Host and Type in ubuntu@ip_address . You should be in!","title":"Connecting VSCodium to your EC2 Instance"},{"location":"vscodium/#code-review-with-vscodium","text":"You can navigate to the repository of interest within VSCodium and any differences in your branch compared to the remote branch will now show up with different color codes as below. This is very handy when trying to get an overall view of differences in images and/or files with multiple differences. Below, on the left, multiple files have changes (M = modified) and on the right, there are differences between the local and remote file (red, deleted lines).","title":"Code Review with VSCodium"}]}