{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation for the Rokita Lab and BTI Bioinformatics Core Welcome to ReadtheDocs for Children's National Hospital Rokita Lab and Brain Tumor Institute (BTI) Bioinformatics team members. This website is a centralized location that contains information about tool accessibility as well as general guidance for utilizing compute infrastructure and using reproducible practices for analytical projects. To file an issue, please see the docs repo here . Responsible and Ethical Use of Bioinformatics Resources Ethical considerations Please be sure to work with your PI and/or Dr. Rokita to understand which metadata, clinical data, and genomic data types are allowed to be stored in public and private spaces. Please do not share our AWS account number or IP addresses externally or within GitHub. If you have questions, please ask us! Submitting jobs or tasks HPC: When utilizing the HPC, please be mindful when submitting large or many jobs, as this is a community resource. AWS: We have created an EC2 service catalog product, \"BTI Research\" for use by researchers in the BTI. Each person can utlize their own instance without affecting others' compute. CAVATICA : We primarily utilize CAVATICA for established upstream workflows. Jobs will be automatically scheduled and queued based on your user capacity. Fiscal Responsibility AWS: Please only launch an EC2 instance type and volume for the work you will need to do to minimize costs (We are charged for instances per hour and by storage). Below are some ways you can minimize costs. Always STOP your instance once you are finished, or if are not actively working on the instance (e.g. you have 3 meetings and will not be working on it - you can stop it and come back later). TERMINATE your instance once you no longer need it, or if you will not be using it for an extended period of time (weeks - months). CAVATICA: Use of CAVATICA requires set up of a billing group and funds. Please work with your PI and/or Dr. Rokita if you would like to run tasks in CAVATICA. The NIH has Cloud Credits available for working with specific datasets through application (e.g. CFDE and/or Kids First ). Cancer Genomics Cloud (CGC) : New users of the CGC get $300 in cloud credits when they create a new account, and this can be used to run and/or test new workflows as desired. CGC has cloud credits available through application as well. Responsible Use of Artificial Intelligence (AI) in Bioinformatics Using AI for bioinformatics (e.g. chatbots, GitHub Copilot) can drastically streamline time to project completion as it can be very useful when converting code from one language to another, developing functions, working out bugs, and more. However, it is very important to consider the information and data we enter into a chatbot or AI assistant such as Github Copilot. - Do not paste any identifiable patient data into chatbots, whether in the form of files or free text. - Do not utilize any AI software in conjuction with GitHub or your text editor so that private repositories are not exposed.","title":"Home"},{"location":"#documentation-for-the-rokita-lab-and-bti-bioinformatics-core","text":"Welcome to ReadtheDocs for Children's National Hospital Rokita Lab and Brain Tumor Institute (BTI) Bioinformatics team members. This website is a centralized location that contains information about tool accessibility as well as general guidance for utilizing compute infrastructure and using reproducible practices for analytical projects. To file an issue, please see the docs repo here .","title":"Documentation for the Rokita Lab and BTI Bioinformatics Core"},{"location":"#responsible-and-ethical-use-of-bioinformatics-resources","text":"","title":"Responsible and Ethical Use of Bioinformatics Resources"},{"location":"#ethical-considerations","text":"Please be sure to work with your PI and/or Dr. Rokita to understand which metadata, clinical data, and genomic data types are allowed to be stored in public and private spaces. Please do not share our AWS account number or IP addresses externally or within GitHub. If you have questions, please ask us!","title":"Ethical considerations"},{"location":"#submitting-jobs-or-tasks","text":"HPC: When utilizing the HPC, please be mindful when submitting large or many jobs, as this is a community resource. AWS: We have created an EC2 service catalog product, \"BTI Research\" for use by researchers in the BTI. Each person can utlize their own instance without affecting others' compute. CAVATICA : We primarily utilize CAVATICA for established upstream workflows. Jobs will be automatically scheduled and queued based on your user capacity.","title":"Submitting jobs or tasks"},{"location":"#fiscal-responsibility","text":"AWS: Please only launch an EC2 instance type and volume for the work you will need to do to minimize costs (We are charged for instances per hour and by storage). Below are some ways you can minimize costs. Always STOP your instance once you are finished, or if are not actively working on the instance (e.g. you have 3 meetings and will not be working on it - you can stop it and come back later). TERMINATE your instance once you no longer need it, or if you will not be using it for an extended period of time (weeks - months). CAVATICA: Use of CAVATICA requires set up of a billing group and funds. Please work with your PI and/or Dr. Rokita if you would like to run tasks in CAVATICA. The NIH has Cloud Credits available for working with specific datasets through application (e.g. CFDE and/or Kids First ). Cancer Genomics Cloud (CGC) : New users of the CGC get $300 in cloud credits when they create a new account, and this can be used to run and/or test new workflows as desired. CGC has cloud credits available through application as well.","title":"Fiscal Responsibility"},{"location":"#responsible-use-of-artificial-intelligence-ai-in-bioinformatics","text":"Using AI for bioinformatics (e.g. chatbots, GitHub Copilot) can drastically streamline time to project completion as it can be very useful when converting code from one language to another, developing functions, working out bugs, and more. However, it is very important to consider the information and data we enter into a chatbot or AI assistant such as Github Copilot. - Do not paste any identifiable patient data into chatbots, whether in the form of files or free text. - Do not utilize any AI software in conjuction with GitHub or your text editor so that private repositories are not exposed.","title":"Responsible Use of Artificial Intelligence (AI) in Bioinformatics"},{"location":"access/","text":"Access to Resources and Tools GitHub We operate two GitHub organizations, the Rokita Lab and BTI Bioinformatics Core . All Rokita Lab and BTI Bioinformatics Core staff should be added to both. High Performance Cluster (HPC) Requesting access While on VPN, fill out this form for HPC access. HPC basics The HPC Wiki can be found here . To view available modules, use module avail . To load modules, use module load xxx To mount L drive folders (e.g. to mount the L Drive folder CancerImmunology-BTI ), follow the instructions below, changing the group to the group to which you belong. #!/bin/bash # Add this file under $HOME/.bashrc.d/ with permission '0640' FOLDER=$HOME/CancerImmunology-BTI CIFS=\"//cnmc.org/CRI_LAB1/CancerImmunology-BTI\" GROUP=\"rokitagrp\" # There settings are generally fine USER=$(whoami) FILE_MODE='0750' DIR_MODE='0750' case $(hostname) in pphpcln*|pphpcdtn* ) if [ ! -d $FOLDER ]; then mkdir -p $FOLDER fi if ! mountpoint -q $FOLDER then echo $FOLDER is not mounted echo Mounting it... sudo /usr/sbin/mount.cifs -o username=${USER},domain=cnmc.org,uid=${USER},gid=${GROUP},dir_mode=${DIR_MODE},file_mode=${FILE_MODE} $CIFS $FOLDER fi esac After sourcing this file, you should be able to see the folder in /home/USER/CancerImmunology-BTI . Amazon Web Services (AWS) For information about our account, please see pinned file in Slack. Requesting access First, obtain email approval from Dr. Jo Lynne Rokita for access to the BTI Bioinformatics AWS account. Open a Service Catalog item in Service Now using the \"Add user to existing Access Group for AWS\" template. AWS account name and number = <see slack for account number> Request ReadOnly access Accessing AWS through the console Once access is granted, you should be able to access our account using CNH SSO credentials using this URL . You will have read access to S3 buckets and EC2 Service Catalog launch permissions. CAVATICA If you do not already have an account, you can generate one using your eCommons ID here . If you do not have an eCommons ID, please work with Dr. Rokita to get one. Self-service password reset On occasion, your password may be reset without warning (!). To reset it, go to this url and follow the instructions. Box We use Box to store various documents for grants, manuscripts, presentations, and collaborator files. Open a Service Catalog item in Service Now and request access to Box for Children's National. Google Drive We favor Google Drive over OneDrive for collaborative manuscript writing and collaborative documentation. Currently, Google Drive is sometimes (or always?) blocked on PCs, but to date, it is available on Apple devices. Paperpile We have paperpile licenses for individuals actively writing manuscripts on Google Drive needing to insert references. Please ask Dr. Rokita for a license.","title":"Technology Access Requests"},{"location":"access/#access-to-resources-and-tools","text":"","title":"Access to Resources and Tools"},{"location":"access/#github","text":"We operate two GitHub organizations, the Rokita Lab and BTI Bioinformatics Core . All Rokita Lab and BTI Bioinformatics Core staff should be added to both.","title":"GitHub"},{"location":"access/#high-performance-cluster-hpc","text":"","title":"High Performance Cluster (HPC)"},{"location":"access/#requesting-access","text":"While on VPN, fill out this form for HPC access.","title":"Requesting access"},{"location":"access/#hpc-basics","text":"The HPC Wiki can be found here . To view available modules, use module avail . To load modules, use module load xxx To mount L drive folders (e.g. to mount the L Drive folder CancerImmunology-BTI ), follow the instructions below, changing the group to the group to which you belong. #!/bin/bash # Add this file under $HOME/.bashrc.d/ with permission '0640' FOLDER=$HOME/CancerImmunology-BTI CIFS=\"//cnmc.org/CRI_LAB1/CancerImmunology-BTI\" GROUP=\"rokitagrp\" # There settings are generally fine USER=$(whoami) FILE_MODE='0750' DIR_MODE='0750' case $(hostname) in pphpcln*|pphpcdtn* ) if [ ! -d $FOLDER ]; then mkdir -p $FOLDER fi if ! mountpoint -q $FOLDER then echo $FOLDER is not mounted echo Mounting it... sudo /usr/sbin/mount.cifs -o username=${USER},domain=cnmc.org,uid=${USER},gid=${GROUP},dir_mode=${DIR_MODE},file_mode=${FILE_MODE} $CIFS $FOLDER fi esac After sourcing this file, you should be able to see the folder in /home/USER/CancerImmunology-BTI .","title":"HPC basics"},{"location":"access/#amazon-web-services-aws","text":"For information about our account, please see pinned file in Slack.","title":"Amazon Web Services (AWS)"},{"location":"access/#requesting-access_1","text":"First, obtain email approval from Dr. Jo Lynne Rokita for access to the BTI Bioinformatics AWS account. Open a Service Catalog item in Service Now using the \"Add user to existing Access Group for AWS\" template. AWS account name and number = <see slack for account number> Request ReadOnly access","title":"Requesting access"},{"location":"access/#accessing-aws-through-the-console","text":"Once access is granted, you should be able to access our account using CNH SSO credentials using this URL . You will have read access to S3 buckets and EC2 Service Catalog launch permissions.","title":"Accessing AWS through the console"},{"location":"access/#cavatica","text":"If you do not already have an account, you can generate one using your eCommons ID here . If you do not have an eCommons ID, please work with Dr. Rokita to get one.","title":"CAVATICA"},{"location":"access/#self-service-password-reset","text":"On occasion, your password may be reset without warning (!). To reset it, go to this url and follow the instructions.","title":"Self-service password reset"},{"location":"access/#box","text":"We use Box to store various documents for grants, manuscripts, presentations, and collaborator files. Open a Service Catalog item in Service Now and request access to Box for Children's National.","title":"Box"},{"location":"access/#google-drive","text":"We favor Google Drive over OneDrive for collaborative manuscript writing and collaborative documentation. Currently, Google Drive is sometimes (or always?) blocked on PCs, but to date, it is available on Apple devices.","title":"Google Drive"},{"location":"access/#paperpile","text":"We have paperpile licenses for individuals actively writing manuscripts on Google Drive needing to insert references. Please ask Dr. Rokita for a license.","title":"Paperpile"},{"location":"aws/","text":"AWS Computing Setting up AWS Single-sign-on (SSO) Make sure you have AWS cli installed on your local machine. aws configure sso SSO session name (Recommended): <anything> SSO start URL [None]: <add the start URL, can be found in slack> SSO region [None]: us-east-1 <Enter> This should open a URL in your browser - accept access CLI default client Region [None]: us-east-1 CLI default output format [None]: CLI profile name [XXX-AccountNumber]: cnh-sso You can now use this new single-sign on profile as aws s3 ls --profile cnh-sso EC2 Instance Types and Launching an EC2 Instance","title":"AWS Compute"},{"location":"aws/#aws-computing","text":"","title":"AWS Computing"},{"location":"aws/#setting-up-aws-single-sign-on-sso","text":"Make sure you have AWS cli installed on your local machine. aws configure sso SSO session name (Recommended): <anything> SSO start URL [None]: <add the start URL, can be found in slack> SSO region [None]: us-east-1 <Enter> This should open a URL in your browser - accept access CLI default client Region [None]: us-east-1 CLI default output format [None]: CLI profile name [XXX-AccountNumber]: cnh-sso You can now use this new single-sign on profile as aws s3 ls --profile cnh-sso","title":"Setting up AWS Single-sign-on (SSO)"},{"location":"aws/#ec2-instance-types-and-launching-an-ec2-instance","text":"","title":"EC2 Instance Types and Launching an EC2 Instance"},{"location":"cavatica/","text":"CAVATICA Computing CAVATICA is a platform which enables users to run AWS compute jobs through Common Workflow Language (CWL). As such, we have to pay for analyses run on CAVATICA and storage of data on CAVATICA which is not mounted to any of our AWS S3 buckets. Keeping this in mind, we will discuss as a team what can be run when and with what money. We will generally use CAVATICA to: Run upstream processing workflows on raw data files, for example, those created through the Kids First Data Resource Center (KF DRC) at CHOP. Benchmark and implement new workflows specific to the needs of the BTI, for example, CITE-Seq, miRNA-Seq, pangenome analyses, etc. Small-scale merges/quick analyses of many data files to avoid downloads, such as through Data Studio. How to mount an AWS volume in CAVATICA In order to mount an AWS volume, you need to have access keys to our AWS account. We will not give these out to everyone, so generally, Alex Sickler and Jo Lynne Rokita will be the ones with keys for mounting. Lordley Okarter, who is the AWS account admin, is the only one who can create these. Once you have keys: Go to Data \u2014> Volumes Follow the instructions for the bucket you would like to mount Copy the IAM policy, save it as a JSON file, and add it via PR to the BTI bucket policy repository . Then, send this JSON file to Lordley Okarter (cc Jo Lynne), as he is currently the only person who can add these to the IAM policy for our AWS account. Once Lordley confirms this was added, you can proceed. Add your access keys. For FIPS, use s3-fips.us-east-1.amazonaws.com For encryption, use default 256 . Note: we usually will mount these as read/write if we desire to export data back to the bucket. Additional documentation on mounting volumes inside of CAVATICA can be found here .","title":"CAVATICA Compute"},{"location":"cavatica/#cavatica-computing","text":"CAVATICA is a platform which enables users to run AWS compute jobs through Common Workflow Language (CWL). As such, we have to pay for analyses run on CAVATICA and storage of data on CAVATICA which is not mounted to any of our AWS S3 buckets. Keeping this in mind, we will discuss as a team what can be run when and with what money. We will generally use CAVATICA to: Run upstream processing workflows on raw data files, for example, those created through the Kids First Data Resource Center (KF DRC) at CHOP. Benchmark and implement new workflows specific to the needs of the BTI, for example, CITE-Seq, miRNA-Seq, pangenome analyses, etc. Small-scale merges/quick analyses of many data files to avoid downloads, such as through Data Studio.","title":"CAVATICA Computing"},{"location":"cavatica/#how-to-mount-an-aws-volume-in-cavatica","text":"In order to mount an AWS volume, you need to have access keys to our AWS account. We will not give these out to everyone, so generally, Alex Sickler and Jo Lynne Rokita will be the ones with keys for mounting. Lordley Okarter, who is the AWS account admin, is the only one who can create these. Once you have keys: Go to Data \u2014> Volumes Follow the instructions for the bucket you would like to mount Copy the IAM policy, save it as a JSON file, and add it via PR to the BTI bucket policy repository . Then, send this JSON file to Lordley Okarter (cc Jo Lynne), as he is currently the only person who can add these to the IAM policy for our AWS account. Once Lordley confirms this was added, you can proceed. Add your access keys. For FIPS, use s3-fips.us-east-1.amazonaws.com For encryption, use default 256 . Note: we usually will mount these as read/write if we desire to export data back to the bucket. Additional documentation on mounting volumes inside of CAVATICA can be found here .","title":"How to mount an AWS volume in CAVATICA"},{"location":"cnh-onboarding/","text":"CNH-specific onboarding The Children's National Intranet (MS Sharepoint site) can be found here and is relatively easy to navigate. Training modules As a staff member who will have access to large amounts of genomic data and potentially protected health information (PHI) for patients, we require everyone complete multiple general staff and specific biomedical research traing modules. CITI If you do not have an account set up with CITI, please make one here . If your account has an old email associated, please update to use your Children's National email address and/or add your affiliation to Children's National Medical Center (CNMC). This way, other staff members can search for you and see whether you had completed certain trainings. Next, you should be able to see a number of courses for CNMC. Please complete the following: Biomedical Research (17 modules) RCR: BIOMEDICAL RESEARCH (6 modules) Conflict of Interest (4 modules) Cornerstone Cornerstone is the place where you will complete CNH-related training/learning modules. From the CNMC Sharepoint, navigate to Resources --> Apps & Tools --> Cornerstone You will periodically see modules assigned. Please complete these before their due dates. Your Timecard Kronos is the system used for recording time. From the CNMC Sharepoint, navigate to Resources --> Apps & Tools --> Cornerstone During your first week, please email Dawn Griffiths and Erica Cleary to set your weekly schedule. They manage all timecards for the Department of Childhood Cancer and Immunology Research (CCIR). Once this is completed, you do not need to fill out your timecard weekly, nor approve it. Once you are eligible to take time off (ASSLA after 90 days and sick/annual leave after 180 days), you will then log into Kronos and be able to request time off in the right-hand screen.","title":"CNH-Specific Onboarding"},{"location":"cnh-onboarding/#cnh-specific-onboarding","text":"The Children's National Intranet (MS Sharepoint site) can be found here and is relatively easy to navigate.","title":"CNH-specific onboarding"},{"location":"cnh-onboarding/#training-modules","text":"As a staff member who will have access to large amounts of genomic data and potentially protected health information (PHI) for patients, we require everyone complete multiple general staff and specific biomedical research traing modules.","title":"Training modules"},{"location":"cnh-onboarding/#citi","text":"If you do not have an account set up with CITI, please make one here . If your account has an old email associated, please update to use your Children's National email address and/or add your affiliation to Children's National Medical Center (CNMC). This way, other staff members can search for you and see whether you had completed certain trainings. Next, you should be able to see a number of courses for CNMC. Please complete the following: Biomedical Research (17 modules) RCR: BIOMEDICAL RESEARCH (6 modules) Conflict of Interest (4 modules)","title":"CITI"},{"location":"cnh-onboarding/#cornerstone","text":"Cornerstone is the place where you will complete CNH-related training/learning modules. From the CNMC Sharepoint, navigate to Resources --> Apps & Tools --> Cornerstone You will periodically see modules assigned. Please complete these before their due dates.","title":"Cornerstone"},{"location":"cnh-onboarding/#your-timecard","text":"Kronos is the system used for recording time. From the CNMC Sharepoint, navigate to Resources --> Apps & Tools --> Cornerstone During your first week, please email Dawn Griffiths and Erica Cleary to set your weekly schedule. They manage all timecards for the Department of Childhood Cancer and Immunology Research (CCIR). Once this is completed, you do not need to fill out your timecard weekly, nor approve it. Once you are eligible to take time off (ASSLA after 90 days and sick/annual leave after 180 days), you will then log into Kronos and be able to request time off in the right-hand screen.","title":"Your Timecard"},{"location":"code-review-guide/","text":"Code Review Guidelines All pull requests will undergo peer review, and we do so while maintaining a culture of positive peer review. Our Code Review model, based on OpenPedCan . The review intends to ensure that a pull request conforms to the following basic requirements: The code is correct. The results can be reproduced in a reasonable amount of time. The results are expected. The documentation describes the purpose, methods, results, input, output, and how to run the code. The code follows basic style guidelines. The code contains comments that explain non-obvious procedures. The analysis is scientifically sound. But, why? To foster a culture of collaboration. To allow junior team members to learn from senior team members , or frequently, vice versa! To spark knowledge transfer, not only of coding practices, but also through project-specific analyses and subject matter. To track our changes and thoughts over time (i.e. this is our virtual lab notebook). To maintain our high standard of reproducibility and rigor. To have all code, figures, and tables ready for future manuscript submission. Rules of thumb: Always rerun the module to ensure it works and outputs can be reproduced identically. Review \u2264 400 lines of code (LOC) at a time Do not review for more than 60 minutes at a time. Take 20 minute breaks in between reviews. Pull requests can be reviewed using GitHub's review interface . Following are the basic guidelines for reviewing pull requests: Note the type of review you performed: did you look over the source code, did you look over the documentation, did you run the source code, did you look at and interpret the results or a combination of these? Suggest modifications or, potentially, directly suggest, but do not commit changes to , the pull request. Explain in detail whether and why the suggested modifications are necessary , and how the modifications should be implemented specifically, so that the developers are able to follow the suggestions without misunderstanding. For any suggestions/comments that are related to the lines that were changed in the particular PR, make the comment underneath the code by clicking the \"+\" next to the code (see below images). Click \"Start a review\" for the first comment, and afterwards, click \"Add single comment\" to keep all comments within the same review. Click \"Finish your review\" once completed. If there are any comments/suggestions that are outside of the code changes this particular PR, for example, any questions about the results, or any lines of code (although not changed) that seemed puzzling, mention them as comments after you click \"Finish your review\". The permalinks of the referenced results or code can be obtained by the following procedure: click the \"Commits\" tab of the pull request, browse the repository at a specific commit that contains the referenced results or code by clicking the <> button of the commit row, go to the referenced file in the repository to get file or line permalinks. If the suggested modifications are extensive refactoring or re-designing without any change on the code behaviors, results or run-time, consider submitting a new issue for the refactoring or re-designing, and discussing the priority and specific implementations in the new issue, after the pull request under review is merged, so that the pull request under review will not be kept open for lengthy discussions and commits on extensive refactoring or re-designing that are out of the scope of the original pull request addressed issue. Before a repository maintainer merges a pull request, there must be at least one affirmative review. If there is any unaddressed criticism or disapproval, a repository maintainer will determine how to proceed and may wait for additional feedback. Merging approved pull requests If working on a collaborative repository, and a pull request is approved, please do not merge it immediately. Instead, wait for the repository maintainer to coordinate with you on when to merge the pull request and whether the pull request should be updated with the latest dev or main branch. The coordination can reduce the workload of developers for checking whether the dev or main branch updates by merging other pull requests will affect their own pull requests. Additional Code Review Resources Fred Hutch Data Science Lab SmartBear Code Review","title":"Code Review Guidelines"},{"location":"code-review-guide/#code-review-guidelines","text":"All pull requests will undergo peer review, and we do so while maintaining a culture of positive peer review.","title":"Code Review Guidelines"},{"location":"code-review-guide/#our-code-review-model-based-on-openpedcan","text":"The review intends to ensure that a pull request conforms to the following basic requirements: The code is correct. The results can be reproduced in a reasonable amount of time. The results are expected. The documentation describes the purpose, methods, results, input, output, and how to run the code. The code follows basic style guidelines. The code contains comments that explain non-obvious procedures. The analysis is scientifically sound. But, why? To foster a culture of collaboration. To allow junior team members to learn from senior team members , or frequently, vice versa! To spark knowledge transfer, not only of coding practices, but also through project-specific analyses and subject matter. To track our changes and thoughts over time (i.e. this is our virtual lab notebook). To maintain our high standard of reproducibility and rigor. To have all code, figures, and tables ready for future manuscript submission.","title":"Our Code Review model, based on OpenPedCan."},{"location":"code-review-guide/#rules-of-thumb","text":"Always rerun the module to ensure it works and outputs can be reproduced identically. Review \u2264 400 lines of code (LOC) at a time Do not review for more than 60 minutes at a time. Take 20 minute breaks in between reviews. Pull requests can be reviewed using GitHub's review interface . Following are the basic guidelines for reviewing pull requests: Note the type of review you performed: did you look over the source code, did you look over the documentation, did you run the source code, did you look at and interpret the results or a combination of these? Suggest modifications or, potentially, directly suggest, but do not commit changes to , the pull request. Explain in detail whether and why the suggested modifications are necessary , and how the modifications should be implemented specifically, so that the developers are able to follow the suggestions without misunderstanding. For any suggestions/comments that are related to the lines that were changed in the particular PR, make the comment underneath the code by clicking the \"+\" next to the code (see below images). Click \"Start a review\" for the first comment, and afterwards, click \"Add single comment\" to keep all comments within the same review. Click \"Finish your review\" once completed. If there are any comments/suggestions that are outside of the code changes this particular PR, for example, any questions about the results, or any lines of code (although not changed) that seemed puzzling, mention them as comments after you click \"Finish your review\". The permalinks of the referenced results or code can be obtained by the following procedure: click the \"Commits\" tab of the pull request, browse the repository at a specific commit that contains the referenced results or code by clicking the <> button of the commit row, go to the referenced file in the repository to get file or line permalinks. If the suggested modifications are extensive refactoring or re-designing without any change on the code behaviors, results or run-time, consider submitting a new issue for the refactoring or re-designing, and discussing the priority and specific implementations in the new issue, after the pull request under review is merged, so that the pull request under review will not be kept open for lengthy discussions and commits on extensive refactoring or re-designing that are out of the scope of the original pull request addressed issue. Before a repository maintainer merges a pull request, there must be at least one affirmative review. If there is any unaddressed criticism or disapproval, a repository maintainer will determine how to proceed and may wait for additional feedback.","title":"Rules of thumb:"},{"location":"code-review-guide/#merging-approved-pull-requests","text":"If working on a collaborative repository, and a pull request is approved, please do not merge it immediately. Instead, wait for the repository maintainer to coordinate with you on when to merge the pull request and whether the pull request should be updated with the latest dev or main branch. The coordination can reduce the workload of developers for checking whether the dev or main branch updates by merging other pull requests will affect their own pull requests.","title":"Merging approved pull requests"},{"location":"code-review-guide/#additional-code-review-resources","text":"Fred Hutch Data Science Lab SmartBear Code Review","title":"Additional Code Review Resources"},{"location":"docker/","text":"Using Docker or Podman for Containerizing Code We will containerize all packages which can be redistributed without licenses using Docker containers. You can work with these using either Docker or Podman . Creating a Dockerfile and Docker Image How to create a docker registry in CAVATICA Create a repository Log into CAVATICA Click on Developer tab -> Docker registry Click + Create repository (top right) Type repository name and choose visibility and click create . Assign admin rights to Jo Lynne Rokita ( harenzaj ) and Alex Sickler ( sicklera ) by clicking on: <Name of Repository>/ Members Docker Login Log into CAVATICA docker registry using CAVATICA credentials docker login [pgc-images.sbgenomics.com](http://pgc-images.sbgenomics.com/) -u <USERNAME> -p <YOUR-AUTH-TOKEN> Build Image docker build -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest . For Mac users with Apple silicon chips (M1-M4 chip), you may need to specify the platform docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest . Pushing the Docker Image docker push pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] where <username>/<repository_name>[:tag] refers to the image that you have already committed Example : docker push pgc-images.sbgenomics.com/naqvia/autopvs1:latest Pulling the Docker Image docker pull pgc-images.sbgenomics.com/<username>/<repository_name>[:tag]:<tagname> Example : docker pull pgc-images.sbgenomics.com/naqvia/autopvs1:latest Updating the Docker Image Make any necessary changes to Dockerfile and run docker build as done previously. Once complete, the output will print Successfully built <image ID> . Tag this image ID to the remote repo and push using these steps: docker tag <image ID> pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] docker push pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] For more, please review the full documentation . For more about Docker using M1/M2/M3 chip macs, ( see here ).","title":"Using Docker or Podman"},{"location":"docker/#using-docker-or-podman-for-containerizing-code","text":"We will containerize all packages which can be redistributed without licenses using Docker containers. You can work with these using either Docker or Podman .","title":"Using Docker or Podman for Containerizing Code"},{"location":"docker/#creating-a-dockerfile-and-docker-image","text":"How to create a docker registry in CAVATICA Create a repository Log into CAVATICA Click on Developer tab -> Docker registry Click + Create repository (top right) Type repository name and choose visibility and click create . Assign admin rights to Jo Lynne Rokita ( harenzaj ) and Alex Sickler ( sicklera ) by clicking on: <Name of Repository>/ Members Docker Login Log into CAVATICA docker registry using CAVATICA credentials docker login [pgc-images.sbgenomics.com](http://pgc-images.sbgenomics.com/) -u <USERNAME> -p <YOUR-AUTH-TOKEN> Build Image docker build -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest . For Mac users with Apple silicon chips (M1-M4 chip), you may need to specify the platform docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] . Example : docker build --platform=linux/amd64 -t pgc-images.sbgenomics.com/naqvia/autopvs1:latest .","title":"Creating a Dockerfile and Docker Image"},{"location":"docker/#pushing-the-docker-image","text":"docker push pgc-images.sbgenomics.com/<username>/<repository_name>[:tag] where <username>/<repository_name>[:tag] refers to the image that you have already committed Example : docker push pgc-images.sbgenomics.com/naqvia/autopvs1:latest","title":"Pushing the Docker Image"},{"location":"docker/#pulling-the-docker-image","text":"docker pull pgc-images.sbgenomics.com/<username>/<repository_name>[:tag]:<tagname> Example : docker pull pgc-images.sbgenomics.com/naqvia/autopvs1:latest","title":"Pulling the Docker Image"},{"location":"docker/#updating-the-docker-image","text":"Make any necessary changes to Dockerfile and run docker build as done previously. Once complete, the output will print Successfully built <image ID> . Tag this image ID to the remote repo and push using these steps: docker tag <image ID> pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] docker push pgc-images.sbgenomics.com/<username>/<repository_name>:[tag] For more, please review the full documentation . For more about Docker using M1/M2/M3 chip macs, ( see here ).","title":"Updating the Docker Image"},{"location":"docs-resources/","text":"How to update documentation This documentation was created with mkdocs.org . To update, please submit a PR to GitHub . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Docs"},{"location":"docs-resources/#how-to-update-documentation","text":"This documentation was created with mkdocs.org . To update, please submit a PR to GitHub .","title":"How to update documentation"},{"location":"docs-resources/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"docs-resources/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"github-general/","text":"Using GitHub For general information on using GitHub, see here . Setting up a local SSH Key and adding to GitHub On your local computer (or EC2 instance for the first time), run the following command to generate your public SSH key. ssh-keygen <enter> <enter> <enter> cat ~/.ssh/id_rsa.pub Add this new SSH key to your profile in GitHub by following these instructions . Setting up a Github repository locally Clone remote Github repo onto local computer using SSH: git clone git@github.com:rokitalab/OpenPedCan-Project-CNH.git Run project Docker image Obtain latest version of project Docker image using Docker or Podman: docker pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest podman pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Run the Docker container locally (be sure to change ): docker run --name <CONTAINER_NAME> --platform linux/amd64 -d -e PASSWORD=pass -p 8787:8787 -v $PWD:/home/rstudio/OpenPedCan-Project-CNH pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Alternatively, the container can be initialized through the Docker dashboard on desktop Once running, a bash shell can be opened into the container to run analyses. For example if <CONTAINER NAME> = openpedcan, run the following command from the root directory: docker exec -ti openpedcan bash from here, you can navigate to the module of interest as follows: cd /home/rstudio/OpenPedCan-Project-CNH/analyses/module-of-interest Download project data Data can typically be downloaded via a download-data.sh shell script in the project root directory: bash download-data.sh This will download all project data from the associated Amazon S3 bucket and create symlinks in data/ to the latest data release version. This command can be re-run to ensure that the most updated files are downloaded. Creating submodules The following is a good resource: https://git-scm.com/book/en/v2/Git-Tools-Submodules Below is documentation from Kelsey Keith at CHOP: ## Git Submodules Followed the instructions in the git book for setting up a submodule, <https://git-scm.com/book/en/v2/Git-Tools-Submodules> ### Toy Example of How to Set up a Submodule *2022-09-02* Setting up a repo as a submodule of another repo. I used two personal private toy repositories `practice` and `old_practice_repository` ```bash git clone https://github.com/kelseykeith/practice.git cd practice git submodule add https://github.com/kelseykeith/old_practice_repository git submodule init git submodule update Go to the other repository, make some changes, and then you can sync them into the other repository it's a submodule of git clone https://github.com/kelseykeith/old_practice_repository cd old_practice_repository echo 'submodule testing' > submodule_test.txt git add submodule_test.txt git commit -m 'using this repo and the practice repository repo to practice adding a git submodule' git push cd /path/to/practice/repository git submodule update --remote --merge Setting up OpenPedCan-analysis as a submodule of documentation so that I can reference OpenPedCan-analysis files cd documentation git checkout main git checkout -b table-update git submodule add https://github.com/PediatricOpenTargets/OpenPedCan-analysis.git # reset submodule to the last v10/v1.0.0 commit git reset --hard c0e5692e2949ad30b8e087ff0ec4e9385bde4b13","title":"Using GitHub"},{"location":"github-general/#using-github","text":"For general information on using GitHub, see here .","title":"Using GitHub"},{"location":"github-general/#setting-up-a-local-ssh-key-and-adding-to-github","text":"On your local computer (or EC2 instance for the first time), run the following command to generate your public SSH key. ssh-keygen <enter> <enter> <enter> cat ~/.ssh/id_rsa.pub Add this new SSH key to your profile in GitHub by following these instructions .","title":"Setting up a local SSH Key and adding to GitHub"},{"location":"github-general/#setting-up-a-github-repository-locally","text":"Clone remote Github repo onto local computer using SSH: git clone git@github.com:rokitalab/OpenPedCan-Project-CNH.git Run project Docker image Obtain latest version of project Docker image using Docker or Podman: docker pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest podman pull pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Run the Docker container locally (be sure to change ): docker run --name <CONTAINER_NAME> --platform linux/amd64 -d -e PASSWORD=pass -p 8787:8787 -v $PWD:/home/rstudio/OpenPedCan-Project-CNH pgc-images.sbgenomics.com/rokita-lab/openpedcanverse:latest Alternatively, the container can be initialized through the Docker dashboard on desktop Once running, a bash shell can be opened into the container to run analyses. For example if <CONTAINER NAME> = openpedcan, run the following command from the root directory: docker exec -ti openpedcan bash from here, you can navigate to the module of interest as follows: cd /home/rstudio/OpenPedCan-Project-CNH/analyses/module-of-interest Download project data Data can typically be downloaded via a download-data.sh shell script in the project root directory: bash download-data.sh This will download all project data from the associated Amazon S3 bucket and create symlinks in data/ to the latest data release version. This command can be re-run to ensure that the most updated files are downloaded.","title":"Setting up a Github repository locally"},{"location":"github-general/#creating-submodules","text":"The following is a good resource: https://git-scm.com/book/en/v2/Git-Tools-Submodules Below is documentation from Kelsey Keith at CHOP: ## Git Submodules Followed the instructions in the git book for setting up a submodule, <https://git-scm.com/book/en/v2/Git-Tools-Submodules> ### Toy Example of How to Set up a Submodule *2022-09-02* Setting up a repo as a submodule of another repo. I used two personal private toy repositories `practice` and `old_practice_repository` ```bash git clone https://github.com/kelseykeith/practice.git cd practice git submodule add https://github.com/kelseykeith/old_practice_repository git submodule init git submodule update Go to the other repository, make some changes, and then you can sync them into the other repository it's a submodule of git clone https://github.com/kelseykeith/old_practice_repository cd old_practice_repository echo 'submodule testing' > submodule_test.txt git add submodule_test.txt git commit -m 'using this repo and the practice repository repo to practice adding a git submodule' git push cd /path/to/practice/repository git submodule update --remote --merge Setting up OpenPedCan-analysis as a submodule of documentation so that I can reference OpenPedCan-analysis files cd documentation git checkout main git checkout -b table-update git submodule add https://github.com/PediatricOpenTargets/OpenPedCan-analysis.git # reset submodule to the last v10/v1.0.0 commit git reset --hard c0e5692e2949ad30b8e087ff0ec4e9385bde4b13","title":"Creating submodules"},{"location":"github-pr-guide/","text":"GitHub Code and Pull Request (PR) Guidelines We prefer the use of tidyverse functions over base R, when possible. The reason for this is so that all developers and reviewers are speaking the same language, making coding and reviewing easier across the team. Additional style guidelines We follow the tidyverse style guide for writing code. Avoid CamelCase unless already in a package function being used. Preparing Code for a Pull Request (PR) and Review - Some Principles Create quantifiable and transparent analysis goals Usually PRs will be linked to an issue (bug fix, new module development, module rerun with new data) or a scientific question we are seeking to answer. Clearly define this ahead of time by submitting an issue to track what you\u2019d like to do/have done or keep this in mind when creating a new branch for a future review. Use informative commit messages \u201cUpdate 01-new-analysis-01.R\u201d does not adequately explain what was done in that commit. It will be harder for you to go back and find where you changed something and harder for the reviewer to understand what was changed in that commit. Something like \u201cfix bug in survival cox ph function\u201d is more informative. Additional (bulleted) information can be added to the within the commit message if more detail is needed. See \u201cWhy good commit messages matter\u201d . Keep PRs \u2264 400 lines of code (LOC) In practice, a review of 200-400 LOC over 60 to 90 minutes should yield 70-90% defect discovery. Therefore, we aim to have PRs within this 200-400 LOC limit. Separate your scripts by purpose, and stack the PRs. Frequently, an analysis module has multiple scripts. Sometimes a module contains a new function. In these cases, you should separate your PRs by script, stacking them in the order in which they should be reviewed. For more on stacking PRs, see below. See our repository structure guidelines. Annotate your code. This should be done throughout the code, explaining each code chunk in a notebook and/or steps for an analysis. Reviewers will find fewer bugs/mistakes when authors read through and annotate their own code before submitting it for review. Add a module README. See OpenPedCan's Documenting Your Analysis . Add a module run script. Usually this is a shell script which runs all of the module scripts in order. Clearly document your goals within the PR template. Remember, a reviewer may not have the background of your project or previous analyses leading up to the PR in question, so it is very helpful to the reviewer when the PR is clearly described. Check out a new branch To make local changes to project repo, checkout a new branch: git switch -c <new-branch> Make desired changes to project repo in new branch (usually one script per PR) Add new analysis module Edit an existing module (add/edit scripts, etc.) Add new packages to project Dockerfile Add data files for new data release Change branch index to reflect any local changes git add <FILE> for any new or edited files or git add -A to add all changed files git rm <FILE> for any deleted files Commit local changes to branch, and includes a message describing what changes were made git commit -m \"message-regarding-changes\" Push commit(s) to remote repository git push origin <new-branch> This will push commits to remote branch new-branch , which will be created if it does not already exist. If authentication is required, the user needs to do the following: Go to github page \u2192 personal account/Settings/Developer settings choose tokens (classic) \u2192 Generate new token Once in the terminal and asked for authentication, use the token as password. Update local git repo To download all changes from remote repo to local repo, use: git fetch To download changes AND merge them into current branch, use: git pull Submitting a Pull Request (PR) Any changes pushed to a project repo branch must be reviewed by team members prior to merging into another branch. This is accomplished by submitting a pull request (PR) that compares the new branch to the one you would like to merge into, and describes changes made. Navigate to remote project repo, and click \u201cPull Requests\u201d: Click \u201cNew pull request\u201d Select two branches to compare. The base branch should be the branch in which changes were made, and the compare branch should be the one into which you would like to merge. For example if you wanted to merge branch v12-tpm into dev : Click \u201cCreate pull request\u201d. This will navigate you to a page where you can provide a brief title and a description that reflects changes. At minimum, you should give details on: 1) changes made to repository and their motivation, 2) what kind of feedback you are seeking from reviewers, and 3) anything else that should be discussed regarding analyses and other changes. If a markdown template is in place (see example below), all questions should be answered prior to submitting PR. Once completed, click \u201cCreate pull request\u201d. The PR should now be visible if you navigate to open PRs. Add reviewers and labels to PR. There is a sidebar on the \u201cOpen a pull request\u201d page and PR page that allows you to choose reviewers and add labels to PR. To add reviewers, click \u201cReviewers\u201d and type in GitHub username of desired reviewer To add labels, click \u201cLabels\u201d and add desired label (e.g., \u201cReady for Review\u201d, \u201cBlocked\u201d, \u201cStacked\u201d). Stacking PRs To stack a PR, checkout the branch on which you would like to stack your next PR: git switch branch-1 Then, from within that branch, and not main/master/dev, create a new branch for the stacked PR: git switch branch-2 Once you create your PR with the second branch, then change the base branch to branch-1 . Now, you should see only the changes in branch-2 in your PR. These PRs either have to be merged in stacked order or they need to be merged backwards. If merging in order, then be sure to delete the branch post merge of each PR. This will automatically trigger changing of the base branch to main or master or dev in preparation for the next PR merge. If merging backwards, such as with data releases, they should all be approved first, and keep note that the number of changed files will get larger, making it harder to track.","title":"GitHub Pull Request Guidelines"},{"location":"github-pr-guide/#github-code-and-pull-request-pr-guidelines","text":"We prefer the use of tidyverse functions over base R, when possible. The reason for this is so that all developers and reviewers are speaking the same language, making coding and reviewing easier across the team. Additional style guidelines We follow the tidyverse style guide for writing code. Avoid CamelCase unless already in a package function being used.","title":"GitHub Code and Pull Request (PR) Guidelines"},{"location":"github-pr-guide/#preparing-code-for-a-pull-request-pr-and-review-some-principles","text":"Create quantifiable and transparent analysis goals Usually PRs will be linked to an issue (bug fix, new module development, module rerun with new data) or a scientific question we are seeking to answer. Clearly define this ahead of time by submitting an issue to track what you\u2019d like to do/have done or keep this in mind when creating a new branch for a future review. Use informative commit messages \u201cUpdate 01-new-analysis-01.R\u201d does not adequately explain what was done in that commit. It will be harder for you to go back and find where you changed something and harder for the reviewer to understand what was changed in that commit. Something like \u201cfix bug in survival cox ph function\u201d is more informative. Additional (bulleted) information can be added to the within the commit message if more detail is needed. See \u201cWhy good commit messages matter\u201d . Keep PRs \u2264 400 lines of code (LOC) In practice, a review of 200-400 LOC over 60 to 90 minutes should yield 70-90% defect discovery. Therefore, we aim to have PRs within this 200-400 LOC limit. Separate your scripts by purpose, and stack the PRs. Frequently, an analysis module has multiple scripts. Sometimes a module contains a new function. In these cases, you should separate your PRs by script, stacking them in the order in which they should be reviewed. For more on stacking PRs, see below. See our repository structure guidelines. Annotate your code. This should be done throughout the code, explaining each code chunk in a notebook and/or steps for an analysis. Reviewers will find fewer bugs/mistakes when authors read through and annotate their own code before submitting it for review. Add a module README. See OpenPedCan's Documenting Your Analysis . Add a module run script. Usually this is a shell script which runs all of the module scripts in order. Clearly document your goals within the PR template. Remember, a reviewer may not have the background of your project or previous analyses leading up to the PR in question, so it is very helpful to the reviewer when the PR is clearly described.","title":"Preparing Code for a Pull Request (PR) and Review - Some Principles"},{"location":"github-pr-guide/#check-out-a-new-branch","text":"To make local changes to project repo, checkout a new branch: git switch -c <new-branch> Make desired changes to project repo in new branch (usually one script per PR) Add new analysis module Edit an existing module (add/edit scripts, etc.) Add new packages to project Dockerfile Add data files for new data release Change branch index to reflect any local changes git add <FILE> for any new or edited files or git add -A to add all changed files git rm <FILE> for any deleted files Commit local changes to branch, and includes a message describing what changes were made git commit -m \"message-regarding-changes\" Push commit(s) to remote repository git push origin <new-branch> This will push commits to remote branch new-branch , which will be created if it does not already exist. If authentication is required, the user needs to do the following: Go to github page \u2192 personal account/Settings/Developer settings choose tokens (classic) \u2192 Generate new token Once in the terminal and asked for authentication, use the token as password.","title":"Check out a new branch"},{"location":"github-pr-guide/#update-local-git-repo","text":"To download all changes from remote repo to local repo, use: git fetch To download changes AND merge them into current branch, use: git pull","title":"Update local git repo"},{"location":"github-pr-guide/#submitting-a-pull-request-pr","text":"Any changes pushed to a project repo branch must be reviewed by team members prior to merging into another branch. This is accomplished by submitting a pull request (PR) that compares the new branch to the one you would like to merge into, and describes changes made. Navigate to remote project repo, and click \u201cPull Requests\u201d: Click \u201cNew pull request\u201d Select two branches to compare. The base branch should be the branch in which changes were made, and the compare branch should be the one into which you would like to merge. For example if you wanted to merge branch v12-tpm into dev : Click \u201cCreate pull request\u201d. This will navigate you to a page where you can provide a brief title and a description that reflects changes. At minimum, you should give details on: 1) changes made to repository and their motivation, 2) what kind of feedback you are seeking from reviewers, and 3) anything else that should be discussed regarding analyses and other changes. If a markdown template is in place (see example below), all questions should be answered prior to submitting PR. Once completed, click \u201cCreate pull request\u201d. The PR should now be visible if you navigate to open PRs. Add reviewers and labels to PR. There is a sidebar on the \u201cOpen a pull request\u201d page and PR page that allows you to choose reviewers and add labels to PR. To add reviewers, click \u201cReviewers\u201d and type in GitHub username of desired reviewer To add labels, click \u201cLabels\u201d and add desired label (e.g., \u201cReady for Review\u201d, \u201cBlocked\u201d, \u201cStacked\u201d).","title":"Submitting a Pull Request (PR)"},{"location":"github-pr-guide/#stacking-prs","text":"To stack a PR, checkout the branch on which you would like to stack your next PR: git switch branch-1 Then, from within that branch, and not main/master/dev, create a new branch for the stacked PR: git switch branch-2 Once you create your PR with the second branch, then change the base branch to branch-1 . Now, you should see only the changes in branch-2 in your PR. These PRs either have to be merged in stacked order or they need to be merged backwards. If merging in order, then be sure to delete the branch post merge of each PR. This will automatically trigger changing of the base branch to main or master or dev in preparation for the next PR merge. If merging backwards, such as with data releases, they should all be approved first, and keep note that the number of changed files will get larger, making it harder to track.","title":"Stacking PRs"},{"location":"github-repo-guide/","text":"GitHub Repository Guidelines In order to facilitate rapid collaboration across many different projects, we have adapted guidelines for our analysis projects, including the repository structure, docker image principles, a pull request model, and code review. Repository guidelines (see OpenPedCan documentation ) Repository Docker Image We set up all of our repositories to use project-specific Docker images containing an instance of RStudio and tidyverse (R > 4.4) from the Rocker project . rocker/tidyverse has already installed many R packages and their dependencies\u2019 apt packages. e.g. the tidyverse package , the devtools package , the rmarkdown package , some R Database Interface packages, the data.table package , the fst package , and the Apache Arrow R package . More on docker here . Repository folder structure Users performing analyses should always refer to the symlinks in the data/ directory and not files within the release folder, as an updated release may be produced before a publication is prepared. The repository folder structure is designed to separate each analysis into its own set of notebooks that are independent of other analyses. Within the analyses directory, create a folder for your analysis. Choose a name that is unique from other analyses and somewhat detailed. For example, instead of gene-expression , choose gene-expression-clustering if you are clustering samples by their gene expression values. You should assume that any data files are in the ../../data directory and that their file names match what the download-data.sh script produces. These files should be read in at their relative path, so that we can re-run analyses if the underlying data change. Files that are primarily graphic should be placed in a plots subdirectory and should adhere to a color palette guide for your project. Files that are primarily tabular results files should be placed in a results subdirectory. Intermediate files that are useful within the processing steps but that do not represent final results should be placed in ../../scratch/ . It is safe to assume that files placed in ../../scratch will be available to all analyses within the same folder. It is not safe to assume that files placed in ../../scratch will be available from analyses in a different folder. An example highlighting a new-analysis directory is shown below. The directory is placed alongside existing analyses within the analyses directory. In this case, the author of the analysis has run their workflows in R Markdown notebooks. This is denoted with the .Rmd suffix. However, the author could have used Jupyter notebooks, R scripts, or another scriptable solution. The author has created a new function or set of functions and placed those into new-function.R which lives in the util folder of the new-analysis folder. The author has produced their output figures as .pdf files. We have a preference for vector graphics as PDF files, though other forms of vector graphics are also appropriate. The results folder contains a tabular summary as a comma separated values file. We expect that the file suffix ( .csv , .tsv ) accurately denotes the format of the added files. The author has also included a README.md ( see Documenting Your Analysis ). OpenPedCan-analysis \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u2502 \u251c\u2500\u2500 existing-analysis-1 \u2502 \u2514\u2500\u2500 new-analysis \u2502 \u251c\u2500\u2500 01-preprocess-data.Rmd \u2502 \u251c\u2500\u2500 02-run-analyses.Rmd \u2502 \u251c\u2500\u2500 03-make-figures.Rmd \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 plots \u2502 \u2502 \u251c\u2500\u2500 figure1.pdf \u2502 \u2502 \u2514\u2500\u2500 figure2.pdf \u2502 \u251c\u2500\u2500 util \u2502 \u2502 \u2514\u2500\u2500 new-function.R \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u2514\u2500\u2500 tabular_summary.csv \u2502 \u2514\u2500\u2500 run-new-analysis.sh \u251c\u2500\u2500 data \u251c\u2500\u2500 download-data.sh \u251c\u2500\u2500 figures \u2514\u2500\u2500 scratch","title":"GitHub Repository Guidelines"},{"location":"github-repo-guide/#github-repository-guidelines","text":"In order to facilitate rapid collaboration across many different projects, we have adapted guidelines for our analysis projects, including the repository structure, docker image principles, a pull request model, and code review.","title":"GitHub Repository Guidelines"},{"location":"github-repo-guide/#repository-guidelines-see-openpedcan-documentation","text":"Repository Docker Image We set up all of our repositories to use project-specific Docker images containing an instance of RStudio and tidyverse (R > 4.4) from the Rocker project . rocker/tidyverse has already installed many R packages and their dependencies\u2019 apt packages. e.g. the tidyverse package , the devtools package , the rmarkdown package , some R Database Interface packages, the data.table package , the fst package , and the Apache Arrow R package . More on docker here . Repository folder structure Users performing analyses should always refer to the symlinks in the data/ directory and not files within the release folder, as an updated release may be produced before a publication is prepared. The repository folder structure is designed to separate each analysis into its own set of notebooks that are independent of other analyses. Within the analyses directory, create a folder for your analysis. Choose a name that is unique from other analyses and somewhat detailed. For example, instead of gene-expression , choose gene-expression-clustering if you are clustering samples by their gene expression values. You should assume that any data files are in the ../../data directory and that their file names match what the download-data.sh script produces. These files should be read in at their relative path, so that we can re-run analyses if the underlying data change. Files that are primarily graphic should be placed in a plots subdirectory and should adhere to a color palette guide for your project. Files that are primarily tabular results files should be placed in a results subdirectory. Intermediate files that are useful within the processing steps but that do not represent final results should be placed in ../../scratch/ . It is safe to assume that files placed in ../../scratch will be available to all analyses within the same folder. It is not safe to assume that files placed in ../../scratch will be available from analyses in a different folder. An example highlighting a new-analysis directory is shown below. The directory is placed alongside existing analyses within the analyses directory. In this case, the author of the analysis has run their workflows in R Markdown notebooks. This is denoted with the .Rmd suffix. However, the author could have used Jupyter notebooks, R scripts, or another scriptable solution. The author has created a new function or set of functions and placed those into new-function.R which lives in the util folder of the new-analysis folder. The author has produced their output figures as .pdf files. We have a preference for vector graphics as PDF files, though other forms of vector graphics are also appropriate. The results folder contains a tabular summary as a comma separated values file. We expect that the file suffix ( .csv , .tsv ) accurately denotes the format of the added files. The author has also included a README.md ( see Documenting Your Analysis ). OpenPedCan-analysis \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u2502 \u251c\u2500\u2500 existing-analysis-1 \u2502 \u2514\u2500\u2500 new-analysis \u2502 \u251c\u2500\u2500 01-preprocess-data.Rmd \u2502 \u251c\u2500\u2500 02-run-analyses.Rmd \u2502 \u251c\u2500\u2500 03-make-figures.Rmd \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 plots \u2502 \u2502 \u251c\u2500\u2500 figure1.pdf \u2502 \u2502 \u2514\u2500\u2500 figure2.pdf \u2502 \u251c\u2500\u2500 util \u2502 \u2502 \u2514\u2500\u2500 new-function.R \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u2514\u2500\u2500 tabular_summary.csv \u2502 \u2514\u2500\u2500 run-new-analysis.sh \u251c\u2500\u2500 data \u251c\u2500\u2500 download-data.sh \u251c\u2500\u2500 figures \u2514\u2500\u2500 scratch","title":"Repository guidelines (see OpenPedCan documentation)"},{"location":"project-mgmt/","text":"BTI Project Management","title":"BTI Project Management"},{"location":"project-mgmt/#bti-project-management","text":"","title":"BTI Project Management"},{"location":"sprints/","text":"Sprints","title":"Sprint Planning"},{"location":"sprints/#sprints","text":"","title":"Sprints"}]}